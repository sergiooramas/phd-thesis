Music content creation, publication and dissemination has changed dramatically in the last few decades. On the one hand, huge amounts of information about music are being daily published in online repositories such as web pages, forums, wikis, and social media. However, most of this content is still unusable by machines due to the fact that it is mostly created by humans and for humans. On the other hand, online music services currently offer ever-growing collections with dozens of millions of music tracks. This vast availability has posed two serious challenges. First, how can a musical item be properly annotated and classified within a large collection? Second, how can a user explore or discover preferred music from all the available content? In this thesis, we address these two questions focusing on the semantic enrichment of descriptions associated to musical items (e.g., artists biographies, album reviews, matadata), and the exploitation of the heterogeneous data in large music collections (e.g., text, audio, images). To this end, we first focus on the problem of linking music-related texts with online knowledge repositories via entity linking, and on the automated construction of music knowledge bases via relation extraction. Then, we investigate how extracted knowledge may impact recommender systems, classification approaches, and musicological studies. We show how modeling semantic information contributes to outperform text-based approaches in artist similarity and music genre classification, and to achieve significant improvements with respect to state of the art collaborative algorithms in music recommendation, while promoting long tail recommendations. Then, we research on learning new data representations from multimodal content using deep learning architectures. Following this approach, we address the problem of cold-start music recommendations by combining audio and text. We show how the semantic enrichment of texts and the late fusion of learned data representations improve the quality of recommendations. Moreover, we tackle the problem of multi-label music genre classification from audio, text, and images. Experiments show major differences between modalities, which not only introduce new baselines for multi-label music genre classification, but also suggest that learning and combining data representations yields superior results. As an outcome of this thesis, we have collected and released six different datasets and two knowledge bases. Our findings can be directly applied to design new algorithms for tasks such as music recommendation, and more specifically to recommend music from novel and unknown artists, which potentially have impact in the music industry. Although our research is motivated by particularities of the music domain, we believe the proposed approaches can be easily generalized to other domains.
