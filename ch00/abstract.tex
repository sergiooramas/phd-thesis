Music content creation, publication and dissemination has changed dramatically in the last few decades. On the one hand, huge amounts of information about music are being daily published in online repositories such as web pages, forums, wikis, and social media. However, most of this content is still unusable by machines due to the fact that it is mostly created by humans and for humans. On the other hand, online music services currently offer ever-growing collections with dozens of millions of music tracks. This vast availability has posed two serious challenges. First, how can a musical item be properly annotated and classified within a large collection? Second, how can a user explore or discover preferred music from all the available content? In this thesis, we address these two questions focusing on the enrichment of item descriptions, and the exploitation of the heterogeneous data in large music collections. We first leverage semantic information present in online repositories. To this end, we focus on the problem of linking musical texts and metadata descriptions with online knowledge repositories via entity linking, and on the automatic construction of music knowledge bases via relation extraction. Then, we investigate how extracted knowledge may impact recommender systems, classification approaches, and musicological studies. We show how modeling semantic information contributes to outperform text-based approaches in music similarity and classification, and to achieve significant improvements with respect to state of the art collaborative algorithms in music recommendation, while promoting long tail recommendations. Then, we research on learning new data representations from multimodal content using deep learning architectures. Following this approach, we address the problem of cold-start music recommendations by combining audio and text. We show how the semantic enrichment of texts and the late fusion of feature embeddings learned separately improve the quality of recommendations. Moreover, we tackle the problem of multi-label music genre classification from different data modalities (i.e. audio, text and images). Experiments show major differences between modalities, which not only introduce new baselines for multi-label genre classification, but also suggest that combining them yields superior results. As an outcome of this thesis, we have collected and released eight different datasets, two knowledge bases, and two software libraries. Our findings have significant implications for music recommendation and classification systems. Although our research is motivated by particularities of the music domain, we believe the proposed approaches can be easily generalized to other domains.
