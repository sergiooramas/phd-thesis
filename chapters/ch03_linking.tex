%!TEX root = ../thesis_a4.tex
\part{Knowledge Extraction}
\label{part:knowledge-extraction}

\chapter[Linking Music Related Texts to Knowledge Bases][Linking Music Related Texts to KBs]{Linking Music Related Texts to Knowledge Bases}
\label{sec:linking}


\section{Introduction}
\label{sec:linking:intro}

In this chapter we focus on the problem of linking texts about music, such as artist biographies or music reviews, to knowledge repositories, such as Wikipedia, DBpedia or MusicBrainz. 
The language used to describe music and its context is specially ideosincratic, and Natural Language Processing (NLP) tools and techniques may not be specifically tuned to it. A first step towards the creation of domain specific NLP tools is the creation of large scale corpus of annotated documents.
However, there is a lack of these music specific datasets for tasks such as named entity recognition or entity linking. Aiming at bridging this gap, we propose \textsc{ELMD}, an automatically constructed corpus where named entities are classified as any of four predefined \textit{musical categories}, namely \textit{Song}, \textit{Album}, \textit{Artist}, and \textit{Record Label}. It was created by leveraging the hyperlinks present in a set of artist biographies gathered from \texttt{Last.fm}\footnote{http://www.last.fm}. Then, we further enrich \textsc{ELMD} by performing entity linking and automatically annotating a large portion of the entities with their DBpedia URI. \textsc{ELVIS} (Entity Linking Voting and Integration System), a voting-based algorithm for entity linking is applied, which considers, for each entity mention in text, the degree of agreement across three state-of-the-art entity linking systems. 
Manual evaluation shows that entity linking Precision is at least 94\% in the resulting dataset.
%The source data used in this chapter comes from the music website and social network Last.fm\footnote{http://www.last.fm}. %To the best of our knowledge, this is the first attempt to provide an annotated large-scale corpus of linked entities in the music domain.
Then, a process to propagate the annotations in \textsc{ELMD} is presented, and annotations are further enriched with MusicBrainz URLs.
Finally, a subset of 200 documents is manually annotated with named entities and MusicBrainz URLs to provide a comprenhensive gold standard dataset. 

%This latter dataset have been uses in the context of the 3rd Open Knowledge Extraction Challenge \cite{TODO}.

%\textsc{ELMD} is built by leveraging hyperlinks appearing in artist biographies collaboratively written by the Last.fm community, together with the integration of three state-of-the art Entity Linking systems. In this way, we produce a highly precise mapping between a Last.fm URL and its corresponding \textsc{DBpedia} URI. 

%The final resource amounts to 47,254 sentences, in which 92,930 entities are categorized into the aforementioned \textit{musical categories}, and 64\% of them are disambiguated and linked to DBpedia. We achieve a Precision score of 97\% in the most restrictive setting, in which our approach manages to annotate more than 31,000 entities.

In the remainder of this chapter, we first introduce \textsc{ELVIS}, our entity linking integration and agreement approach (Section~\ref{sec:linking:elvis}). Then, we describe the text corpus we compiled from the \texttt{Last.fm} website and how it is combined with ELVIS (Section~\ref{sec:linking:lastfm}). In the next step, the obtained dataset is evaluated (Section~\ref{sec:linking:eval}). Then, a further process of automatic expansion of \textsc{ELMD} is describled (Section~\ref{sec:linking:extending}). Finally, the manual annotation of a subset of \textsc{ELMD} is presented (Section~\ref{sec:linking:gold}), and some conclusions are drawn (Section~\ref{sec:linking:conclusions}). %Finally, we describe the resulting output of our system: The \textsc{ELMD} dataset. 
%A summary of our approach is provided in Figure \ref{fig:linking:workflow}.


\section{Music entity linking}
\label{sec:linking:el}

%When we refer to the Music Domain in a Natural Language Processing (NLP) context we refer to Music product reviews such as albums or songs, artist biographies or even song lyrics. While these are valuable resources in NLP for tasks like Sentiment Analysis, Music Information Retrieval (MIR), however, has barely exploited the information and knowledge that can be extracted from textual data. This opens up a vibrant area of research where MIR tasks may benefit dramatically from mining textual data. 

Named entity recognition is the task to identify mentions to entities belonging to a set of predefined categories \citep{ZhouandJian2002}. Traditionally, the most widely covered types of entities are \textsc{Person}, \textsc{Location} and \textsc{Organization}, as well as numeric expressions or time-spans. While named entity recognition is a widely studied topic, and has been at the core of well-known shared tasks and conferences \citep{Nadeau2007} such as MUC, ACE or CoNLL, the advent of large knowledge repositories and collaborative resources has contributed to the emergence of another discipline: entity linking, i.e. to discover mentions of entities in text and link them to a suitable knowledge repository \citep{Moroetal2014}.

In many circumstances, it may be useful to obtain annotations for Music entity mentions in text, either simply as Music types (e.g. tagging `Yellow Submarine' as \texttt{Song}) or performing entity linking, e.g. tagging `Yellow Submarine' as \texttt{dbpedia.org/page/Yellow\_Submarine\_(song)}. However, this is not a trivial task as mentions to Music entities show language and register idiosyncrasies \citep{TataandDiEugenio2010,Gruhl2009}, and therefore a certain degree of tailoring is required in order to account for them. Let us consider, for instance, multiword Music entities, which usually are those who pose greatest challenges for EL. As \cite{TataandDiEugenio2010} point out, they are difficult to discover because they may not be restricted to a single Noun Phrase or may be abbreviated (by means of acronyms, dropping entire words or even full rephrasing). Additionally, a specific trait of Music texts is the fact that one song may have many covers by many different artists and, according to our evaluation, it may be difficult even for a human to identify what \textit{version} of the song the writer is referring to. Furthermore, availability of entity linking testbeds in general \citep{Usbeck2015}, and in the Music domain in particular \citep{Gruhl2009}, is scarce, making it very difficult to evaluate novel systems and approaches. Hence, it is difficult to know how well a certain method, which may work well for generic texts, will perform on Music data.

%The advent of large Knowledge Repositories or Knowledge Bases (KBs) and collaborative resources has contributed to the emergence of entity linking (EL), 
%%%i.e. the task of discovering mentions of entities in text and link them to a suitable KB \citep{Moroetal2014}. 
%In the context of Open Data, there is an increasing need for benchmarking and evaluation corpora. However, while general purpose datasets exist , dealing with highly specific domains (e.g. chemistry) or ever-evolving areas (e.g. videogames or music) poses a greater challenge for entity linking due to linguistic idiosyncrasies or under-representation in general purpose KBs. The latter is precisely the case of the music domain , where available evaluation data for entity linking is scarce.

Despite the current context of scarcity of both entity linking systems and evaluation benchmarks in the Music domain, there are some exceptional cases in which these issues were addressed, such as: (1) Detecting Music entities (e.g. songs or bands) on informal text \citep{Gruhl2009}; (2) Applying Hidden Markov Models for discovering Music entity mentions in Chinese corpora \citep{Zhang2009}.%; or (3) Recognizing musical entities in the context of a relation extraction pipeline \cite{Oramas2015}. 

A large number of entity linking systems which are not bound to any domain or discipline have emerged in the last years. However, we have observed that the number of identification errors in musical entities produced by these systems is still high. We argue that this problem of precision may be tackled by leveraging a combination of several of these generic entity linking off-the-shelf systems. Simply put, we hypothesize that if two or more generic systems annotate with the same URI an entity mention, the probability of this annotation to be correct increases. To the best of our knowledge, very little effort has been put in exploiting this \textit{agreement} feature. One of the reasons may be that, as of now, most entity linking systems \textit{speak their own language}, partially due to the fact that each of them points back to different KBs, and hence their output is heterogeneous and cannot be directly compared, let alone combine. This has motivated research towards unification frameworks for evaluation of EL. For instance, \cite{Cornolti2013} put forward a benchmarking framework for comparing entity linking systems. Moreover, \cite{Rizzo2014} describe a system aimed at combining the output of the different named entity recognition systems. Finally \cite{Usbeck2015} present \textsc{GERBIL}, an evaluation framework for semantic entity linking based on \cite{Cornolti2013}. %This is an open-source and extensible framework that allows evaluating tools against different datasets.



\section{ELVIS}
\label{sec:linking:elvis}

In this section we describe \textsc{ELVIS}, the generic integration framework for entity linking, which is leveraged for the construction of \textsc{ELMD}. First, we describe our entity linking research problem and provide an intuition on how this may be surmounted via an agreement scheme. Then, we provide details on the main modules integrating \textsc{ELVIS}, highlighting the possible cases of agreement and disagreement over the entity linking systems that are integrated in our framework.

\subsection{Argumentum ad populum in entity linking}
\label{sec:linking:agreement}

Our method relies on the \textit{argumentum ad populum} intuition, i.e. if two or more different entity linking systems perform the same prediction in linking a named entity mention to its entry in a reference KB, the more likely this prediction is to be correct. We put this intuition into practice by combining the output of three well-known systems, namely DBpedia Spotlight~\citep{Mendes2011} , Tagme~\citep{Ferragina2012} and Babelfy~\citep{Moroetal2014b}, whose agreement (or disagreement) when disambiguating an in-text entity mention is taken as an agreement-driven \textit{confidence score}. These specific tools were chosen for being considered state-of-the-art entity linking systems and for being well known in the NLP community. However, \textsc{ELVIS} can easily incorporate any additional system. We also selected these tools because entities identified by all of them can be easily referenced to DBpedia URIs. 
While these tools have proven highly competitive on their own, in this chapter we explore the gain in performance obtained by combining them together, and apply global agreement-driven decisions on the \texttt{Last.fm} corpus.
%Moreover, although there are other knowledge bases (e.g. MusicBrainz) with substantially more musical entities than DBpedia, to the best of our knowledge, there is no entity linking tool that works with these domain specific knowledge bases. 


\subsection{`Translating' entity linking formats}
\label{sec:linking:unification}

In order to have each entity linking system \textit{speak the same language} for measuring agreement in their predictions, output homogenization is required. This is not a trivial task, as each entity linking approach may be based on a different reference KB, the offsets may be computed differently, and so on. For instance, DBpedia Spotlight links entity mentions via DBpedia URIs, whereas Tagme provides Wikipedia page IDs, and Babelfy disambiguates against BabelNet \citep{NavigliPonzetto2012} and its corresponding BabelNet synsets. 
%Thus, we aim at exploiting the simple but powerful intuition that the more an arbitrary number of entity linking systems agree in their decision when disambiguating a named entity, the more likely this decision is to be correct. We therefore propose an agreement-based approach and apply it to the raw \texttt{Last.fm} corpus.
%Since our agreement-based approach requires a homogenization of the outputs, 
We attempt to surmount this heterogeneity as follows: First, we retrieve DBpedia URIs of every named entity. There are some considerations to be taken into account, however: (1) Character encoding differs from system to system, which we address by converting the character encoding of the retrieved URI to \textsc{UTF-8}; (2) Several URIs may refer to the same DBpedia resource. We solve this specific issue thanks to the transitive redirections provided by DBpedia. If a URI has a transitive redirection, it is replaced by the redirected URI. (3) Note that, in the case of Tagme, only Wikipedia page IDs are provided, which we can straightforwardly exploit to map entity mentions to their DBpedia equivalent. Finally, and after surmounting compatibility issues among systems, we retrieve DBpedia types (\texttt{rdf:type} property) and Wikipedia categories (\texttt{dcterms:subject} property) for all entities. This \textit{type} information is further used in the creation of \textsc{ELMD}.
%The source code of this entity linking unification framework is released as open source.

%and quotation -> "character encoding AND QUOTATION(?)"

%\subsection{entity linking Agreement}
%\label{sec:agreement}

After successfully providing a process which harmonizes the output of entity linking systems, it is possible to compute the degree of agreement among them, which will become our system's confidence score. We define the following set of \textit{agreement heuristics} to set such score for each linking prediction (an overview of the workflow of \textsc{ELVIS} is provided in Figure \ref{fig:linking:workflow}).

\begin{itemize}
    \item \textbf{Full Agreement $(++)$} When all systems detect an entity with the same URI and offset.
    \item \textbf{Partial Agreement $(+)$} When more than one but less than all systems detect an entity with the same URI and offset. Outliers (i.e. systems performing a different prediction) may detect a different entity or may not detect anything.
    \item \textbf{Singleton Decision $(-)$} When only one system detects an entity for a given text offset.
    \item \textbf{Disagreement $(--)$} When more than one system performs a linking over the same text offset, but all of their predictions are different.
\end{itemize}

%If all the three systems detect an entity with the same URI and offset, our system's confidence is 3, whereas confidence is set to 2 when only two of them agree (the remaining system may not detect anything or detect a different entity). However, if only one system detects an entity in a given text snippet, our confidence score is 1 (we refer to these entities as \textit{singletons}). Finally, we classify as \textit{disagreement} those cases where more than one system detects something in a given snippet but there is no agreement. 
%%%The output of the agreement step follows the same data structure as the output of the unification step, but adding to every entity the confidence score and the name of the tool or tools involved in the agreement. In case of disagreement, the different URIs and DBpedia types from every tool are provided.


\begin{figure}[h!]
  \centering
	\includegraphics[height=3.25cm,width=8cm]{ch03_linking_pics/workflow_bn.pdf}
  \caption{ELVIS Workflow.}
  \label{fig:linking:workflow}
\end{figure}


\section{From \texttt{Last.fm} to \textsc{ELMD}}
\label{sec:linking:lastfm}

In what follows, we describe the original data gathered from \texttt{Last.fm}, and the process to apply the integration framework described in Section \ref{sec:linking:elvis}, in order to construct a highly precise benchmarking dataset for entity linking in the Music domain.


In \texttt{Last.fm}, users may add relevant biographical details to any artist's main page in the form of a \textit{wiki}. These edits are regularly moderated. Furthermore, artist biographies are often enriched with hyperlinks to other \texttt{Last.fm} Artist, Album, Song and Record Label pages, similarly as with Wikipedia hyperlinks. Our purpose is to leverage this meta-information to automatically construct a dataset of Music-specific annotated named entities.% and for evalauting the soundness of \textsc{ELVIS}.\\

We crawled artist biographies from \texttt{Last.fm} in March 2015, and gathered 13,000 artist biographies, which comprise 47,254 sentences with at least one hyperlink, amounting to a total of 92,930 links. These may be broken down as follows: (1) 64,873 hyperlinks referencing Artist pages; (2) 16,302 to Albums; (3) 8,275 to Song pages; and finally (4) 3,480 hyperlinks referencing Record Labels. This \textit{type} information is extracted thanks to the structure of each link's URL, as it includes in its path the category of the annotated entity. Consider, for example, the following sentence:

\begin{adjustwidth}{2.5em}{20pt}
\begin{center}
After their debut The Intelligence got signed to \textit{In the Red Records}.\\
\end{center}
\end{adjustwidth}


Here, we may infer that the entity \textit{In the Red Records} is a Record Label, thanks to its \texttt{Last.fm} URL: {\footnotesize{\texttt{http://www.last.fm/{\normalsize\textbf{label}}/In+the+Red+Records}}}. This information is extracted from the whole \texttt{Last.fm} corpus for those entities falling in one of the four \textit{musical categories} previously defined.

%%%\begin{figure}[h!]
%%%  \centering
%%%	\includegraphics[height=3.5cm]{elmd_creator.png}
%%%  \caption{Dataset creation.}
%%% \label{fig:linking:dataset_creator}
%%%\end{figure}

%After this extraction process, a first version of the dataset is built with the identified and categorized named entities.



\subsection{Data enrichment}
\label{sec:linking:enrichment}

For the creation of the ELMD dataset, the crowdsourced annotations extracted from \texttt{Last.fm} biographies are combined with decisions made by \textsc{ELVIS} and its voting framework.\\%(see Figure~\ref{fig:linking:dataset_creator}).

Every entity mention annotated in the \texttt{Last.fm} corpus is a candidate to be included in \textsc{ELMD}. The challenge is to assign to each entity its correct DBpedia URI. We approach this problem by leveraging (1) The DBpedia URI assigned by \textsc{ELVIS}, (2) The \textit{agreement score} for that prediction, as well as (3) The \textit{type} information derived from the entity's \texttt{Last.fm} URL. Our intuition is that the higher the \textit{agreement score}, the more likely the prediction is to be correct. Likewise, we also hypothesize that if a linking decision made by \textsc{ELVIS} coincides in \textit{type} with the original \texttt{Last.fm} annotation, it is more likely to be correct. Since there is no direct mapping between \texttt{Last.fm} and DBpedia types, we manually set the type equivalences shown in Table \ref{tbl:linking:equivalence}.\\

Regarding the \textit{agreement score}, it corresponds to the number of systems that agreed in a decision (see \textbf{Score} column in Table \ref{tbl:linking:examples}). Note that an \textit{agreement score} of 1 may be caused either by cases in which only one system detected an entity mention, or when there is disagreement among systems, but one and only one of them coincides in \textit{type} with the original \texttt{Last.fm} annotation (last row in Table \ref{tbl:linking:examples}).

As for \textit{type equivalence}, this is a binary value (\textit{type-equivalent} or \textit{type-discrepant}) based on coinciding types between \texttt{Last.fm} URLs and \textsc{ELVIS} decisions.
%Note that agreement across systems within \textsc{ELVIS} is not considered (for instance, systems in \textsc{ELVIS} coincide in the example given in the third row of Table \ref{tbl:linking:examples}, but its \textit{type value} is set to \texttt{discrepant} since it does not match with the entity's \texttt{Last.fm} type).

%Additionally, when an entity is considered as disagreement by the agreement framework, but there is type equivalence with  one of the three entity linking systems, we change its confidence score from 0 to 1, and consider it also as a \textit{singleton} (there is an example of this special case in last row of Table~\ref{tbl:linking:examples}).\\

%To measure the usefulness of the type information, we divide the evaluation in two settings namely: (1) \textit{Type equivalence}, when the predicted and target entities share an equivalent type; and (2) \textit{Type nonequivalence}, when this is not the case. Note that for \textit{type nonequivalence} we consider both cases in which \texttt{Last.fm} and DBpedia types do not coincide, or when type information is absent in the DBpedia resource. 

%added to the ELMD dataset along with two features, namely its corresponding offset and \textit{type} information. Then, \textsc{ELVIS} is used to provide a linking between these entities and DBpedia as follows: For each detected entity, if the offset of the \texttt{Last.fm} annotation and an \textsc{ELVIS} prediction coincide, we leverage the \textit{confidence score} provided by the agreement among systems integrating \textsc{ELVIS} together with the entity type (see Table \ref{tbl:linking:equivalence}).% and use it as a confidence booster. %Therefore, there are two parameters that are used in the decision process, the confidence score and the type equivalence flag.


%provides a high precision linking between these entities and DBpedia. For each annotated entity, we obtain two important pieces of information: (1) \textsc{ELVIS} confidence score, which is obtained after applying the aforementioned \textit{agreement heuristics}; and (2) \textit{type} information, obtained from the original \texttt{Last.fm} data. 


%if there is an entity in the agreement output with the same offset, we analyze its confidence score and type information to decide whether to take its URI or not.
%We compare the type information gathered from DBpedia with the music categories inferred from the original \texttt{Last.fm} data 

%If our system considers both type information as equivalent, the 

%Additionally, in the type agreement setting, we consider as \textit{singleton} instead of disagreement, decisions cases in which 
%more than one system detects something in a text snippet, and they predict different entities, but 
%only one system predicts the correct type of the entity, giving to this entities a confidence score of 1.

%We acknowledge that, while this additional information may be helpful for enriching the \texttt{Last.fm} corpus, it does not reflect a realistic scenario in which we would perform entity linking on unseen data. 


%In addition to system agreements, we apply a music-specific heuristic exploiting music categories inferred from the original \texttt{Last.fm} data. Specifically, we use the \textit{music type} information provided through the rdf:type property of DBpedia resources as an agreement booster. This means that if the agreed URI contains a type that equivalent to the category of the candidate entity (see Table \ref{tbl:linking:equivalence}), we consider this as a case of \textit{type agreement}. For every level of system agreement, we differentiate the cases where there is type agreement from the other cases. In addition, when there is disagreement between systems, but only one of them detects an entity with type agreement, our confidence score is 1, like for singletons.

\begin{table}
\small
\centering
\def\arraystretch{1.2}
	\begin{tabular}{ l l }
\hline
\textbf{Last.fm type} & \textbf{DBpedia type} \\
\hline
Song & DBpedia:Song, DBpedia:Single, Yago:Song \\
Album & \parbox[t]{5cm}{DBpedia:Album, Yago:Album,\\ Schema:MusicAlbum} \\
Artist & \parbox[t]{5cm}{DBpedia:MusicalArtist, \\DBpedia:Band, Schema:MusicGroup, Yago:Musician, Yago:Creator, DBpedia:Artist} \\
Record Label & DBpedia:RecordLabel \\
\hline
	\end{tabular}
	\caption{Equivalence of types between Last.fm and DBpedia.}
	\label{tbl:linking:equivalence}
\end{table}

\begin{table*}[!t]
\scriptsize
\def\arraystretch{1.2}
\centering
	\begin{tabular}{| p{2cm} | c | L{1.8cm} | L{1.8cm} | L{1.5cm} | c | L{1cm} |}
\hline
\textbf{Context} & \textbf{Type} & \textbf{Tagme} & \textbf{Babelfy} & \textbf{Spotlight} & \textbf{Score} & \textbf{Type Eq.} \\
\hline
and the academic minimalism of \textbf{Steve Reich} & Artist &  Steve\_Reich (type:artist) & Steve\_Reich (type:artist) & Steve\_Reich (type:artist) & 3 & yes \\
\hline
The new album \textbf{Hypocrisy} followed shortly thereafter & Album & --- & Hypocrisy (type:band) & Hypocrisy (type:band) & 2 & no \\
\hline
The third album \textbf{Lucifer Songs}, opened new and unexpected doors & Album & --- & Lucifer\_Songs (type:album) & --- & 1 & yes \\
\hline
The band's debut album, \textbf{Cookies}, was released on 14 May 2007 & Album & HTTP\_cookie {\footnotesize{(type:unk)}} & Cookies (type:album) & --- & 1 & \parbox[t]{1cm}{\centering yes\\ (only\\ Babelfy)} \\
\hline
	\end{tabular}
	\caption{Agreement examples of ELVIS.}
	\label{tbl:linking:examples}
\end{table*}

%\subsection{Unification Framework for Entity Linking}

%\subsection{Agreement-based Entity Linking Confidence}

\begin{figure}[!t]
  \centering
	\includegraphics[width=\textwidth]{ch03_linking_pics/evaluation_both_bn.png}
  \caption[Number of entities and precision of the manual evaluation.]{Number of entities and precision of the manual evaluation.  Note the major differences in Precision between \textit{type-equivalent} and \textit{type-discrepant} systems.}
  \label{fig:linking:agreement_typed}
\end{figure}

\begin{table}[]
\centering
\def\arraystretch{1.2}
	\begin{tabular}{ c  c  c  c  c }
\hline
 & \textbf{Agreement} & \textbf{Precision} & \textbf{No. Entities} \\
\hline
\multirow{3}{*}{type-equivalent} & $=3$ & 0.97 & 31,180\\
&$\geq2$ & 0.96 & 46,544 \\
&$\geq1$ & 0.94 & 59,680 \\
\hline
%\multirow{3}{*}{type equivalence + type nonequivalence} & $=3$ & 0.94 & 33,455\\
\multirow{3}{*}{all} & $=3$ & 0.94 & 33,455\\
&$\geq2$ & 0.90 & 51,802\\
&$\geq1$ & 0.81 & 72.365\\
\hline
	\end{tabular}
	\caption[Precision and coverage of ELMD mapping to DBpedia.]{Precision and number of entities with this value of precision of ELMD mapping to DBpedia. \textit{Type-equivalent} implies entities from the type-equivalent configuration only, whilst \textit{All} implies all entities regardless their type information.}
	\label{tbl:linking:results}
\end{table}

\section{Evaluation}
\label{sec:linking:eval}

Considering the different possibilities of agreement across the three systems integrating \textsc{ELVIS}, there are in total 7 possible configurations: 1 with \textbf{full agreement} (score$=3$); 3 with \textbf{partial agreement} (score $=2$); and 3 \textbf{singleton} configurations (score$=1$). Moreover, considering also the two possible values of \textit{type equivalence}, namely \texttt{equivalent} and \texttt{discrepant}, we have a total number of 14 configurations. Figure \ref{fig:linking:agreement_typed} provides a visual overview of these configurations, where we show both Precision scores for each configuration (in bold) in addition to the number of entities disambiguated with \textsc{ELVIS} in each case.

%Considering the two possible values of \textit{type agreement}, namely \texttt{equivalent} and \texttt{discrepant}, and the different possibilities of agreement across the systems integrating \textsc{ELVIS}, there are in total 14 possible configurations. These may be broken down into 7 configurations with \textit{type equivalence} and 7 configurations with \textit{type discrepancy}. These 7 configurations according to Entity Linking agreement: 1 \textbf{full agreement} configuration (score$=3$); 3 \textbf{partial agreement} configurations (score $=2$); and 3 \textbf{singleton} configurations  (score$=1$). Figure \ref{fig:linking:agreement_typed} provides a visual overview of these configurations, where we show both Precision scores for each configuration (in bold) in addition to the number of entities disambiguated with \textsc{ELVIS} in each case.\\

\begin{table}[]
\scriptsize
\small
\centering
\def\arraystretch{1.2}
	\begin{tabular}{l r r r l}
\hline
\textbf{Category} & \textbf{Annotations} & \textbf{Entities} & \textbf{Avg-words} & \textbf{Most frequent} \\
\hline
Song & 3,302 & 2,823 & 2.81 & Shine (6) \\
Album & 7,872 & 6,897 & 2.69 & Like Drawing Blood (6) \\
Artist & 46,337 & 17,535 & 1.88 & The Beatles (160) \\
Label & 2,169 & 815 & 1.94 & Sub Pop (33) \\
\hline
	\end{tabular}
	\caption[Statistics of the linked entities in ELMD.]{Statistics of the linked entities in ELMD. We report, for each \textit{musical category}, the total number of annotations linked to DBpedia, number of unique entities, average number of words per entity mention, and most frequently annotated entity (along with its frequency).}
	\label{tbl:linking:statistics}
\end{table}

%(1 with \textbf{full agreement} 3, 3 with \textbf{partial agreement}, and 3 with confidence 1) with \textit{type equivalence}, and 7 with \textit{type nonequivalence}. These 14 combinations are illustrated in Figure~\ref{fig:linking:agreement_typed}. 

We evaluated 100 randomly selected entity samples (25 for each of the four Music categories we consider) from each one of the 14 possible configurations, and asked an eva\-luator with computational linguistics background to manually assess the correctness of the 1,400 predictions. From scores obtained from manual evaluation, we estimated Precision for the whole \textsc{ELMD} dataset with different ranges of \textit{agreement score} as well as two options \textit{type}-wise (see Table~\ref{tbl:linking:results}). The precision value for all the entities is computed proportionally according to the number of entities and the precision obtained in the manual evaluation for the \textit{type-equivalent} and \textit{type-discrepant} settings, hence these can be seen as Micro Average Precision numbers.

We observe that the \textit{type-equivalent} configuration yields much better Precision with only a slight tradeoff in terms of coverage. Therefore, we decided to select for the final \textsc{ELMD} dataset only those URIs stemming from a \textit{type-equivalent} setting where \textit{agreement score} is equal or greater to 1. This ensures a Precision of at least 0,94 in terms of entity linking. Moreover, a manual survey of false positives in the highest scoring setting (\textit{agreement score}$=3$ and \textit{type-equivalent}) showed that these are cases in which even a human annotator may not find it trivial to correctly find the correct entity to those entity mentions. One of these cases are those in which \textsc{ELVIS} is presented with and entity mention that on surface may refer to either an Artist or an Album named after the artist or band itself. An actual case of false positive in our evaluation dataset is the following sentence:
\begin{adjustwidth}{2.5em}{20pt}
\begin{center}
Her debut album , \textit{Kim Wilde}, (released on RAK records) came out in July 1981 and stayed in the U.K. album charts for 14 weeks, peaking at number 3 and getting much acclaim.
\end{center}
\end{adjustwidth}

Here, the entity \textit{Kim Wilde} should be disambiguated as the Album with the same name as the artist, but \textsc{ELVIS} incorrectly assigned the Artist's DBpedia URI: {\footnotesize{\texttt{dbpedia.org/resource/Kim\_Wilde}}}. In \textsc{ELMD} there are 50 cases where the same surface text is correctly linked to an Artist entity in some sentences, and to a Song entity in others. Similar ambiguous cases involving Artist and Album (148) and Song and Album (95) are correctly resolved by our system. These particularly challenging cases may be interesting for training Music specific entity linking algorithms.

Another interesting source of false positives comes between musical entities and equally named entities (not necessarily related to Music). In cases in which the latter are more popular in a reference KB, e.g. their associated node in the graph may have higher connectivity, may become prioritized by disambiguation entity linking algorithms that consider graph connectivity as a feature. Consider the following sentence:

\begin{adjustwidth}{2.5em}{20pt}
\begin{center}
He is becoming more and more in demand for his remixing skills; working for the likes of Justin Timberlake and Armand van Helden, and labels including \textit{Ministry Of Sound}, Defected and Intec, to name a just a few.
\end{center}
\end{adjustwidth}

Here, the entity \textit{Ministry of Sound} refers to a Record Label, a spin-off of the well-known club, which is the entity that was incorrectly assigned: {\footnotesize{\texttt{dbpedia.org/resource/Ministry\_of\_Sound}}}. 
Cases like this would require, first, to ensure that the different entities derived from \textit{Ministry of Sound} (such as the Record Label or a clothing brand of the same name) exist in a reference KB, and second, to exploit contextual information so that a correct decision is made.
A similar situation happens when song or album names may be confused with very common words or expressions (e.g. `Easy', `Stupid', `Sad song', `If', `Be there'). \textsc{ELMD} is rich in challenging cases like these.

%It would be very difficult, even for human annotators to obtain higher Precision. %[EJEMPLO DE CASO CONCRETO]

%We randomly selected a sample of 100 entities from each one of the 14 sets of entities defined in Figures~\ref{fig:linking:agreement_typed} and~\ref{fig:linking:agreement_nontyped}. In Figure~\ref{fig:linking:agreement_typed} are represented the entities with type agreement, whereas in Figure~\ref{fig:linking:agreement_nontyped} are represented the ones where the condition of agreement was not satisfied, whether because there is disagreement between the \texttt{Last.fm} type and the DBpedia type, or because there is not enough type information in the DBpedia resource. Thus, a total number of 1,400 entities were picked up, and the linking was manually evaluated by a linguist with annotation experience. From the precision levels obtained in the manual evaluation, we estimated the precision and recall of the different combinations of agreement for the whole dataset (see Table~\ref{tbl:linking:results}).

%Describe how evaluation data was generated

%Give numbers and discuss them (maybe include the figure by Sergio)

%Hihglight the fact that errors of type-A3 are actually impossible to detect even for a human.

\begin{figure}[h!]
  \centering
	\includegraphics[width=8cm]{ch03_linking_pics/ELMD_Overview_bn.pdf}
  \caption{ELMD Overview. Number of entities, confidence score and precision values in different subsets of the dataset.}
  \label{fig:linking:elmd}
\end{figure}

\section{Extending ELMD}
\label{sec:linking:extending}

The number of links present in the \texttt{Last.fm} biographies is small compared to the size of the biographies. For instance, a link may have been added only once in a specific biography, even though the same entity is mentioned several times along the text. 
In addition, music information represented in DBpedia is not complete, as many existing artists, albums, and songs does not have a Wikipedia page. As a consequence of that, there are many annotated links in the biographies to \texttt{Last.fm} pages that does not have a corresponding DBpedia resource.
Therefore, to extend the coverage and the number of annotations of the \textsc{ELMD} dataset we applied the following processes. First, we take advantage of the fact that a large portion of \texttt{Last.fm} annotations have a direct mapping to MusicBrainz, and this information can be retrieved through the \texttt{Last.fm} API. Thus, in addition to the already available DBpedia links, MusicBrainz URLs are added to the annotations, when this information is available. Furthermore, existing annotations in every document are propagated, assuming they appear in a one-sense-per-discourse fashion \citep{gale1992one}. For example, if the text span \textit{The Beatles} is marked as an annotation in the first sentence of a document, and it appears again in the second sentence, but there is no annotation associated, an annotation is added. Finally, we look for mentions of the entity that constitutes the main theme of the biography, and annotate all its mentions within the biography, assuming unambiguity. The number of annotations and distinct entities after the extension process are reported in Table~\ref{tbl:linking:elmd2}. Note that MB has a coverage of 93.6\% over all the annotations, and 91.1\% over all distinct entities (see Table~\ref{tbl:linking:elmd2_percentage}).

\begin{table}[]
%\def\arraystretch{1.25}
\centering
\begin{tabular}{ l r r }
\hline
& \textbf{Annotations} & \textbf{Entities} \\ \hline
All    & 144,593      & 63,902    \\ 
Artist & 112,524      & 39,131    \\ 
Album  & 18,701       & 15,064    \\ 
Song  & 9,203        & 7,832     \\ 
Label  & 4,165        & 1,875    
\\ \hline
\end{tabular}
\caption[Statistics of the extended ELMD corpus.]{Statistics of the extended ELMD corpus. \texttt{Annotations} refers to all distinct mentions or apparitions of an entity of its corresponding type, whereas the \texttt{Entities} column refers to the number of distinct entities of each type.}
\label{tbl:linking:elmd2}
\end{table}

\begin{table}[]
%\def\arraystretch{1.25}
\centering
\begin{tabular}{ l r r }
\hline
& \textbf{Annotations} & \textbf{Entities} \\ \hline
DBpedia    & 58.6\%      & 49.1\%    \\ 
MusicBrainz & 93.6\%      & 91.1\%    \\
Both  & 57.2\%       & 47\%    \\ 
None & 5\%	& 9.2\% \\ \hline
\end{tabular}
\caption{Percentage of linked entities in the extended ELMD corpus.}
\label{tbl:linking:elmd2_percentage}
\end{table}


\section{Gold standard dataset}
\label{sec:linking:gold}

We envision a wide range of potential applications for \textsc{ELMD}, such as acting as a training set for a named entity recognition or entity linking system, or as an evaluation benchmark. However, it suffers from two major problems that differentiate it from a gold standard dataset which undergous a full manual validation pass. First, although there is an important number of annotated entities, there are still many musical entities mentioned in \textsc{ELMD} texts that are not linked to any KB, nor even annotated. Second, as the dataset has been automatically generated, it is prune to errors, as we show in the evaluation in Section \ref{sec:linking:eval}. To tackle these issues, a subset of 200 documents from \textsc{ELMD} was manually annotated by a human expert. We asked the annotator to mark in each document all mentions of entities of the following types: \textit{Artist}, \textit{Album} and \textit{Song}. Record Label entities were discarded due to the low number of annotations present in the documents. In addition, the annotator manually searched for each entity in the MusicBrainz database. If it was present, the MusicBrainz URL was added to the annotation. The final number of annotations is shown in Table~\ref{tbl:linking:ELMDGold}.
This gold standard dataset has been used in the Task 3 of the third edition of the Open Knowledge Extraction Challenge, co-located with the Extended Semantic Web Conference (ESWC 2017) \citep{}.

\begin{table}[]
%\def\arraystretch{1.25}
\centering
\begin{tabular}{l r r }
\hline
& \textbf{Annotations} & \textbf{Entities} \\ \hline
All    & 5,184      & 2,803    \\ 
Artist & 3,828      & 1,926    \\ 
Album  & 860       & 693    \\ 
Song  & 496        & 184     \\ \hline
\end{tabular}
\caption[Statistics of the ELMD gold standard corpus.]{Statistics of the ELMD gold standard corpus. \texttt{Annotations} refers to all distinct mentions or apparitions of an entity of its corresponding type, whereas the \texttt{Entities} column refers to the number of distinct entities of each type.}
\label{tbl:linking:ELMDGold}
\end{table}


\section{Conclusion}
\label{sec:linking:conclusions}

In this chapter we have described several contributions related to the problem of recognizing and linking musical entities in naturally occurring text. First, for the task of entity linking, we have presented an integration framework called \textsc{ELVIS} which, based on a voting procedure which leverages decisions made by an arbitrary number of off-the-shelf entity linking systems, provides high confident entity disambiguations. Currently, \textsc{ELVIS} incorporates three state-of-the-art systems, namely DBpedia Spotlight, Tagme and Babelfy, and can be easily extended with additional systems. %The \textit{ELVIS} code is available at {\footnotesize{https://github.com/sergiooramas/elvis}}. 
Then, we have leveraged the potential of \textsc{ELVIS} for the creation of a novel benchmarking dataset for entity linking in the Music domain, called \textsc{ELMD}. This corpus comes from a collection of \texttt{Last.fm} artist biographies, and contains 47,254 sentences with 92,930 annotated and classified entity mentions. %(64,873 Artists, 16,302 Albums, 8,275 Songs and 3,480 Record Labels). 
%In addition, by setting up a higher confidence threshold it is possible to obtain a subset of \textsc{ELMD} that prioritize higher Precision by sacrificing Recall (see Figure~\ref{fig:linking:elmd}). 
From this set of entity mentions, 59,680 are linked to DBpedia (see Table~\ref{tbl:linking:statistics}), with a precision of at least 0,94 (see Figure~\ref{fig:linking:elmd}).
Moreover, we have extended the number of annotated entities in ELMD via several heuristics. Furthermore, in addition to the DBpedia linking, we successfully linked 93\% of the annotations to MusicBrainz by leveraging the \texttt{Last.fm} API.
Finally, we have manually annotated and linked to MusicBrainz a gold standard subset of 200 documents from ELMD, for its use within an entity linking challenge.
%\textit{ELVIS}\footnote{https://github.com/sergiooramas/elvis} source code and the different versions of \textsc{ELMD}\footnote{http://mtg.upf.edu/download/datasets/elmd} have been released.
