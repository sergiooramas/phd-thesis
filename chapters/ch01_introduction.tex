%!TEX root = ../thesis_a4.tex

\chapter{Introduction}
\label{sec:intro}

\section{Motivation}
\label{sec:intro:motivation}

Today, we are witnessing an unprecedented information explosion thanks to the dramatic technological advancement brought by the Information Age. This technological (r)evolution has set the foundations for the release and publication of huge amounts of data onto online repositories such as web pages, forums, wikis and social media. Art and culture have benefited dramatically from this context, which allows potentially anyone with an available Internet connection to access, produce, publish, comment or interact with any form of media. 

In this context, music content creation, publication and dissemination has changed dramatically. Online music services, such as Pandora, Spotify or Apple Music, benefit from this situation and currently offer ever-growing catalogs with dozens of millions of music tracks, which are in turn just one click away from hundreds of millions of users. This vast availability of music has posed two serious challenges: (1) how can a musical item be properly annotated and classified within a large collection? Since manually managing these large libraries is not feasible due to size constraints, automatic methods for the annotation and classification of large-scale music collections have been an active area of research in recent years \cite{schedl2014music}. (2) how can a user explore or discover preferred music from all the available content? Traditionally, users have relied on their friends, their favourite music radio host, a music expert in their local retail store, etc. to obtain recommendations on artists or albums they might like. Although this traditional approach is still valid and used by many people, its ability to cover the vast amount of available music nowadays is seriously hindered. Therefore, automatic approaches to music recommendation have become necessary \cite{celma2008new}.

Large music collections combine information from multiple data modalities, such as audio, images, text or videos. In addition, music collections can be enriched with user generated content published online on a daily basis. However, most of this content is still unusable by machines due to the fact that it is mostly created by humans and for humans, and hence it only exists in human readable form. In this context, Natural Language Processing plays a key role, as one of its main lines of research is precisely to transform unstructured information in machine readable data \cite{cowie1996information}, allowing discovery of new facts and trends hidden in, for example, Music libraries, blogs, web pages, journals or social networks.

The way multimodal data from large music collections is represented and combined in computational models poses numerous challenges. Artificial intelligence methods, such as machine learning, heavily rely on the choice of data representation. Therefore, finding representations that maximize the different explanatory factors of variation behind the data is a fundamental task. Traditional approaches rely on handcrafted features to represent the variability of the data, whereas more recently, and thanks the raise of deep learning techniques, representation-learning approaches have demonstrated their superiority in multiple domains \cite{bengio2013representation}.

In this thesis, we focus on the problem of how to enrich and exploit multimodal data present in large music collections from two different stand points. (1) Leveraging semantic information present in online knowledge repositories and user generated content. (2) Learning new data representations from heterogeneous data using deep learning architectures and further combining these representations in multimodal networks. Both ideas are in turn applied to the aforementioned problems of classification and recommendation of musical items.


\section{Processing Language in Music Information Research}
\label{sec:intro:nlp}

Music Information Retrieval/Research (MIR) is a multidisciplinary field of research concerned with the extraction, analysis, and usage of information about any kind of music entity (e.g. song, artist, album) on any representation level (e.g. audio signal, symbolic MIDI, metadata) \cite{schedl2008}. As stated in \cite{Schedl2013}, factors that influence human music perception can be categorized into music content, music context, user context and user properties. Music context relates to all musical aspects that are not encoded in the audio signal, such as song lyrics, artist's biography, album cover artwork or music video clips, whereas music content is defined as human perceptual aspects that can be extracted from the audio signal. Following this distinction, research methodologies within the MIR community that deals with data modalities different from audio are often called context-based approaches. 
Although we agree with this classification criteria, in concordance with the nomenclature used within the Recommender Systems community \cite{Ostuni2013}, in this dissertation either audio signal, text (e.g. metadata, artist's biographies, song lyrics), images (e.g. album cover artwork, artist's photographies), and video (e.g. music video clips) are simply considered as different modalities of content information.

According to \cite{humphrey2012}, MIR approaches are typically based on a two-stage architecture of feature extraction and semantic interpretation, e.g., classification, regression, clustering, similarity ranking, etc. 
Traditionally, MIR has been mainly focused on the use of features extracted from audio, underestimating other data modalities. However, in recent years several studies have shown the benefits of using \textit{context-based} and multimodal approaches \cite{Schedl2014}. 

Audio features are often classified into low, mid and high-level representations \cite{bello2015}. Low-level representations (e.g. spectral flux, cepstrum, MFCCs) are measured directly on the audio signal. Mid-level representations (e.g. chords, onsets) represent musical attributes extracted from the audio combining machine learning and musical knowledge. High-level representations (e.g. mood, form, genre) are related to human interpretations of the data, and are typically built on top of low and mid-level representations. The extraction and exploitation of features from these three representation levels have been widely studied. 

Following this feature hierarchy, when dealing with textual data, we can also differentiate between low, mid and high-level representations (see Figure~\ref{}). Low-level representations (e.g. word frequencies, word co-occurrences, n-grams) are measured directly on text. Mid-level representations (e.g. part-of-speech tags, named entities) combine linguistic knowledge and statistical analysis of text corpora. High-level representations (e.g. syntactic dependencies, semantic relations) involves a semantic understanding of text. In the context of MIR, most of the literature is focused on low-level representations, few in mid-level, and almost none in high-level. Little attention has been paid in the semantic of words, nor in the context they are being used. Thus, the epistemic potential of text has not been exploited yet.

%Early work in NLP in the context of MIR is related to the extraction of music artist information from artist-related web pages, using search engines to gather those pages and then parsing their DOM trees \cite{Cohen2000}. Other studies \cite{Ellis2002} \cite{Whitman2002} use weighted term profiles based on specific term sets for recommendation and classification tasks. Co-occurrence of artist names in web pages content and page count based on results provided by search engines have been used for artist similarity and recommendation tasks \cite{Schedl2005}. Song lyrics \cite{} and tweets \cite{} are other commonly used text sources in MIR. The number of publications related to this topic have been increasing along the years \cite{}. Two comprenhensive reviews on \textit{context-based} approaches can be found in \cite{Knees2013, Schedl2014}.
%Another interesting application of NLP is the analysis of music artist-related microblogging posts for artist similarity estimation and artist labeling \cite{Schedl2013a}. More de

%There have been also some initial atempts to work with mid and high-level text representations in the context of MIR. In \cite{Sordo2012} a methodology for extracting semantic information from music-related forums is proposed, inferring semantic relations from the co-occurrence of musical concepts in forum posts. 
%In \cite{Sordo2013} a set of semantic facets is automatically obtained and anchored upon the structure of Wikipedia, and tags from the folkosonomy of Last.fm are then categorized with respect to the obtained facets. 
%In \cite{Knees2011} a methodology to automatically extract semantic information and relations about musical entities from arbitrary textual sources is proposed. In \cite{TataandDiEugenio2010} a method to extract information about indidual songs from album reviews is proposed, combining syntactic, semantic and sentiment analysis. Finally, the C@amerata task \cite{sutcliffe2016c, sutcliffe2015}, part of the MeidaEval evaluation campaigns from 2013 to 2017, is focused on music Question & Answering (Q&A) systems. In this task the input is a natural language phrase, together with a music score in MusicXML, and the required output should be one or more matching passages in the score.

In the first and second parts of this thesis, we focus on text-based approaches from two standpoints. On the one hand, we work on new methodologies for the extraction of high level semantic representations from musical texts. On the other hand, we put the emphasis on the development of approaches that exploits these semantic representations in MIR tasks, such as music recommendation and classification. In addition, we study how semantic information may impact musicological studies.

\section{Representation Learning in Music Information Research}
\label{sec:intro:learning}

As stated before, MIR approaches are commonly based on a two-stage architecture of feature extraction and semantic interpretation. In this context, data representations are generally obtained following a traditional feature extraction process, which involves a combination of music domain-knowledge, psychoacoustics, and audio engineering. 
Feature engineering compensate the inability of traditional machine learning algorithms to extract the discriminative information of the data. However, it involves a labor-intensive human effort, and also all the different explanatory factors of variation behind the data are not represented \cite{bengio2013representation}. 
Huge efforts have been put in the last two decades in the definition and extraction of audio features, which has given rise to comprehensive software libraries that assemble many of these feature extraction techniques \cite{Essentia, Librosa}. 

Representation learning (or feature learning) is a technique that allows a learning system to automatically discover the variation behind the data directly from raw data. As identified in \cite{humphrey2012}, MIR approaches can benefit from the use of these learning approaches using deep neural networks. This methodology has two main advantages. First, blurring the boundaries between the two-stage architecture, which implies fully-automated optimization of both stages at once. Second, it results in general-purpose architectures that can be applied to different MIR problems and data modalities. In the last years, several works have been published where end-to-end learning approaches using deep learning architectures have been applied to MIR tasks such as music recommendation \cite{Oord2013} and music classification \cite{Choi2016}, among others.

In the third part of this dissertation, we focus on representation learning approaches using deep neural networks. We apply this methodology to different data modalities (audio, text and images) and their combination, and in the context of music recommendation and classification tasks.


\section{Annotation and Classification of Music Collections}
\label{sec:intro:annotation}

The advent of large music collections has posed the challenge of how to access the information - in terms of retrieval, browsing, and recommendation -. One way to ease the access of large music collections is to keep annotations of all music resources \cite{sordo2012semantic}. Annotations can be added either manually or automatically. Manual annotation of huge music collections is too costly due to high human effort required. Therefore, the implementation of automatic annotations processes has become mandatory. 

We distinguish two ways of automatically enhance annotations: (i) gathering metadata from external sources, and (ii) learning metadata from the collection's data. To address (i), information can be obtained from online knowledge repositories (e.g. Wikipedia, MusicBrainz), or extracted from collections of unstructured documents. This imposes the challenge of how to properly map collection's items with external entities. To address (ii), machine learning techniques can be applied over the collection's data. When annotations are learned from audio this task is often called auto-tagging. However, metadata may be learned from data of different modalities, such as album cover artwork, tags, editorial metadata, user generated content, video clips, etc.

Among the different categories of annotations used in music collections, the most prototypical are: music genres, instruments, and moods. 
Music genre labels are useful categories to organize and classify songs, albums and artists into broader groups that share similar musical characteristics. Music genres have been widely used for music classification, from physical music stores to streaming services. Automatic music genre classification thus is a widely explored topic \citep{sturm2012survey}.
However, almost all related work is concentrated in the classification of music items into broad genres (e.g., Pop, Rock), assigning a single label per item. This is problematic since there may be hundreds of more specific music genres \cite{pachet2000taxonomy}, and these may not be necessarily mutually exclusive (i.e., a song could be Pop, and at the same time have elements from Deep House and a Reggae grove). 

In this thesis, we focus on the problem of enriching metadata in music collections from the two above defined standpoints, i.e. gathering and learning. We study how semantic technologies may be useful to improve the annotations of musical items. In addition, we tackle the problem of single-label and multi-label music genre classification from different data modalities (i.e. audio, text and images) and their combination.


\section{Music Recommendation}
\label{sec:intro:recommendation}

Information overload in modern Web applications challenges users in their decision-making tasks. Recommender systems have emerged in the last years as fundamental tools in assisting users to find, in a personalized manner, what is relevant for them in overflowing knowledge spaces. 

Music Recommendation is a relatively young but continuously growing research topic, in both MIR and RecSys communities~\cite{oscarBook}. Several research approaches and commercial systems have been proposed in the last decade. However, many of them are adaptations from other domains \cite{oscarBook}. 
Music has its own specificities with respect to other domains. For instance, a user may consume a musical item several times, or very different items according to the user context (e.g. working, dinning, excercising). Therefore, music recommendation is a challenging and still unsolved problem.

Although music online services make available almost all existing music, only a small percentage of these catalogues is actually consumed by the vast majority of users. Music consumption follows what is called a long tail distribution \cite{oscarBook} (see Figure~\ref{}). Therefore, one of the main challenges in music recommendation is how to make this long tail of musical items profitable. Moreover, as music creation is continuously growing, new artists and releases appear every day. Hence, another important challenge in recommender systems is how to deal with these new items, which is often called the cold-start problem.

The web is full of documents, knowledge repositories and user generated content with relevant information about music and musicians. This information may have the potential to impact in the performance of music recommender systems. In addition, up to now, audio content has been barely exploited in comercial recommender systems. However, thanks to the advent of novel deep learning approaches \cite{Oord2013}, audio content is becoming a key factor in order to provide accurate long tail recommendations.

Most research in Music Recommendation has been dedicated to developing algorithms that provide \textit{good} and \textit{useful} recommendations~\cite{oscarBook}, neglecting the importance of the diversity or serendipity of recommendations \cite{TODO}. In addition, very few approaches are able to provide explanations of the recommendation to the users~\cite{Passant2008, Passant2010}. According to~\cite{celma2008new}, giving explanations of the recommendations provides transparency to the recommendation process and increases the confidence of the user in the system.

In this thesis we dig into the Music Recommendation problem from three different perspectives. First, we investigate how information extracted from large datasets of documents \textit{talking} about music may be useful to provide explanations of recommendations to users. Second, we tackle the problem of recommending long tail items by leveraging semantic information from knowledge repositories and combining it with users feedback data. Finally, we address the problem of cold-start music recommendations by combining different data modalities in a deep neural network.


\section{Objectives and outline of the thesis}
\label{sec:intro:objectives}

In the previous sections we have explained the motivations and context of our thesis. According to that, the main goal of this dissertation is to contribute to advancing the state of the art in music recommendation and classification by leveraging knowledge repositories and unstructured text sources, and learning novel data representations from multimodal data. Although this thesis is focused on the music domain, the work we present can be easily adapted to other multimedia domains. Figure\ref{} shows a conceptual organisation of some of the chapters of this thesis according to the different approaches, tasks, and the involved level of learning.

This thesis is structured as follows: Chapter~\ref{sec:SOA} presents some background knowledge and related work on Knowledge Extraction and MIR. Hereafter, the work in this dissertation is divided in three Parts: In Part~\ref{part:knowledge-extraction} we explore different techniques and approaches to extract semantic information from unstructured text sources \textit{talking} about music. Within this Part, Chapter~\ref{sec:linking} illustrates the problem of linking musical texts and knowledge repositories. In Chapter~\ref{sec:kb} we address the automatic generation of Music Knowledge Bases from unstructured text sources. This Chapter encloses with an experiment on explanations of music recommendations based on an extracted Knowledge Base. In Chapter~\ref{sec:musicology}, three experiments that study the potential impact of knowledge extraction techniques in musicological studies are presented.
Then, in Part~\ref{part:knowledge-based}, the semantic representations described in Part~\ref{part:knowledge-extraction} are exploited in Music Classification, Similarity and Recommendation problems. Chapter~\ref{sec:similarity} presents the application of a semantic enrichment methodology in music similarity and classification problems, whereas Chapter~\ref{sec:graph-rec} address the problem of long tail recommendations by enriching annotations with semantic information.
Then, in Part~\ref{part:multimodal-deep} an approach to learn data representations from different data modalities using deep neural networks is applied to the Music Recommendation and Classification problems. In Chapter~\ref{sec:cold-rec} we address the problem of cold-start music recommendations using audio and text. Finally, in Chapter~\ref{sec:multimodal-class} we apply a similar approach to music genre classification, and two experiments are presented: a single-label classification problem over audio and images, and a multi-label classification problem over audio, text and images.
At the end of each chapter, we include a focused discussion about the relevant results and conclusions. We conclude this thesis in Chapter~\ref{sec:conclusion} with a summary of our work, our main conclusions, and a discussion about open issues and future perspectives.




