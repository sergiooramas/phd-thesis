%!TEX root = ../thesis_a4.tex

\chapter{Introduction}
\label{sec:intro}

\section{Motivation}
\label{sec:intro:motivation}

Today, we are witnessing an unprecedented information explosion thanks to the dramatic technological advancement brought by the Information Age. This technological (r)evolution has set the foundations for the release and publication of huge amounts of data onto online repositories such as web pages, forums, wikis and social media. Art and culture have benefited dramatically from this context, which allows potentially anyone with an available Internet connection to access, produce, publish, comment or interact with any form of media. 
In this context, music content creation, publication and dissemination has changed dramatically. 
Online music services, such as Pandora, Spotify or Apple Music, benefit from this situation and currently offer ever-growing catalogs with dozens of millions of music tracks, which are in turn just one click away from hundreds of millions of users. This vast availability of music has posed two serious challenges: (1) how can a musical item be properly annotated and classified within a large collection? Since manually managing these large libraries is not feasible due to size constraints, automatic methods for the annotation and classification of large-scale music collections have been an active area of research in recent years. (2) how can a user explore or discover preferred music from all the available content? Traditionally, users have relied on their friends, their favorite music radio host, a music expert in their local retail store, etc. to obtain recommendations on artists or albums they might like. Although this traditional approach is still valid and used by many people, its ability to cover the vast amount of available music nowadays is seriously hindered. Therefore, automatic approaches to music recommendation have become necessary.

Large music collections combine information from multiple data modalities, such as audio, images, text or videos. In addition, music collections can be enriched with user generated content published online on a daily basis. However, most of this content is still unusable by machines due to the fact that it is mostly created by humans and for humans, and hence it only exists in human readable form. In this context, Natural Language Processing plays a key role, as one of its main lines of research is precisely to transform unstructured information in machine readable data, allowing discovery of new facts and trends hidden in, for example, Music libraries, blogs, web pages, journals or social networks.

The way multimodal data from large music collections is represented and combined in computational pipelines poses numerous challenges. Artificial intelligence methods, such as machine learning, heavily rely on the choice of data representation. Therefore, finding representations that maximize the different explanatory factors of variation behind the data is a fundamental task. Traditional approaches rely on handcrafted features to represent the variability of the data, whereas more recently, and thanks the raise of deep learning techniques, representation-learning approaches have demonstrated their superiority in multiple domains.

In this thesis, we focus on the problem of how to enrich and exploit the multimodal data present in large music collections from two different stand points. (1) Leveraging semantic information present in online knowledge repositories and user generated content. (2) Learning new data representations from heterogeneous data using deep learning approaches and further combining these learned representations in multimodal networks. Both ideas are in turn applied to the aforementioned problems of classification and recommendation of musical items.


%\section{Natural Language Understanding}

%Natural language understanding (NLU) is a subtopic of Natural Language Processing (NLP) that deals with machine reading comprehension.
%Knowledge Representation and Reasoning is a key enabler of Intelligent Systems \cite{Suchaneketal2007}, and plays an important role in Natural language Understanding (NLU) \cite{BaralandDeGiacomo2015}.
%In this disertation, we focus on an important aspect of NLU, which is \textit{how to make sense} of the data that is generated and published online on a daily basis. This data is mostly produced in human-readable format, which makes it unsuitable for automatic processing. Considering that deep understanding of natural language by machines seems to be very far off \cite{CambriaandWhite2014}, there is great interest in formalizing unstructured data, and Knowledge Bases (\textsc{KBs}) are a paradigmatic example of large-scale content processed to make it machine readable.

%Information Extraction (IE) is the task of automatically extracting structured information from unstructured or semi-structured text sources. It is a widely studied technique within the Natural Language Processing (NLP) research community \cite{cowie1996information}.
%A major step towards understanding language is the extraction of meaningful terms (entities) from text as well as relationships between those entities. This statement involves two different tasks. (1) to determine the identity and category of entity mentions present in text. This task is called Named Entity Recognition (NER). However, when this task involves a latter step of disambiguation of entities against a KB it is often called Named Entity Disambiguation (NED) or Entity Linking (EL). (2) to identify and annotate relevant semantic relations between entities in text. This task is called Rellation Extraction.

%The work described in this thesis strongly focuses on the exploitation of linguistic and semantic properties of text collections. For this reason, we deem relevant to cover the following areas: (1) \textsc{KB} construction and curation; (2) Music \textsc{KBs}; (3) Entity Linking, (4) Relation Extraction, and (5) Sentiment Analysis.

\section{Processing Language in Music Information Research}

Music Information Retrieval/Research (MIR) is a multidisciplinary field of research that is concerned with the extraction, analysis, and usage of information about any kind of music entity (e.g. song, artist, album) on any representation level (e.g. audio signal, symbolic MIDI, metadata) \cite{schedl2008}. As stated by Schedl in \cite{Schedl2013}, factors that influence human music perception can be categorized into music content, music context, user context and user properties. Music context relates to all musical aspects that are not encoded in the audio signal, such as song lyrics, artist's biography, album cover artwork or music video clips, whereas music content is defined as human perceptual aspects that can be extracted from the audio signal. Following this distinction, research methodologies within the MIR community that deals with data modalities different from audio are often called context-based approaches. 
Although we agree with this classification criteria, in concordance with the nomemclature used within the Recommender Systems community \cite{Ostuni2013}, in this dissertation either audio signal, text (e.g. metadata, artist's biographies, song lyrics), images (e.g. album cover artwork, artist's photographies), and video (e.g. music video clips) are simply considered as different modalities of content information.

%Traditionally, MIR has been mainly focused on the use of features extracted from audio, underestimating other data modalities. However, in recent years several studies have showed the benefits of using \textit{context-based} and multimodal approaches \cite{Schedl2014}. However, most of these works use only shallow text features, such as word counts and co-occurrences, and few of them use also part-of-speach tags. They do not put much attention in the semantic of words, nor in the context they are being used. Therefore, the epistemic potential of text has not been exploited yet.
Traditionally, MIR has been mainly focused on the use of features extracted from audio, underestimating other data modalities. However, in recent years several studies have shown the benefits of using \textit{context-based} and multimodal approaches \cite{Schedl2014}. 
According to \cite{humphrey2012}, MIR approaches are typicaly based on a two-stage architecture of feature extraction and semantic interpretation, e.g., classification, regression, clustering, similarity ranking, etc. As stated before, in the vast majority of cases, features are extracted from audio signals. This audio features are often classified into low, mid and high-level representations \cite{bello2015}. Low-level representations (e.g. spectral flux, cepstrum, MFCCs) are measured directly on the audio signal. Mid-level representations (e.g. chords, onsets) represent musical attributes extracted from the audio combining machine learning and musical knowledge. High-level representations (e.g. mood, form, genre) are related to human interpretations of the data, and are typically built on top of low and mid-level representations. The extraction and exploitation of features from these three representation levels have been widely studied. 

Following this feature hierarchy, when dealing with textual data, we can also differentiate between low, mid and high-level representations (see Figure~\ref{}). Low-level representations (e.g. word frequencies, word co-occurrences, n-grams) are measured directly on text. Mid-level representations (e.g. part-of-speach tags, named entities) combine linguistic knowledge and statistical analysis of text corpora. High-level representations (e.g. syntactic dependencies, semantic relations) involves a semantic understanding of text. In the context of MIR, most of the works found in the literature are focused on low-level representations, few in mid-level representations, and almost none in high-level representations. Little attention has been paid in the semantic of words, nor in the context they are being used. Thus, the epistemic potential of text has not been exploited yet.


%The most frequent approach applied to MIR problems consists on the extraction of hand-crafted features from audio signals to be used in machine learning models able to produce some predictions. The employed audio features are classified as low level (e.g. ) or high level (e.g. ) (see Figure~\ref{}). There are many works in the literature dealing with both kind of feature levels. The same paradigm can be applied to the use of text within MIR. We can obtain shallow or high level semantic features from text, and then use these features within the MIR task. However, most of the works in the literature that exploit text sources are focused on shallow text features, such as word counts, word co-occurrences or part-of-speach-tags. They do not put much attention in the semantic of words, nor in the context they are being used. Therefore, the epistemic potential of text has not been exploited yet.

Early work in NLP in the context of MIR is related to the extraction of music artist information from artist-related web pages, using search engines to gather those pages and then parsing their DOM trees \cite{Cohen2000}. Other studies \cite{Ellis2002} \cite{Whitman2002} use weighted term profiles based on specific term sets for recommendation and classification tasks. Co-occurrence of artist names in web pages content and page count based on results provided by search engines have been used for artist similarity and recommendation tasks \cite{Schedl2005}. Song lyrics and tweets are other commonly used text sources in MIR. The number of publications related to this topic have been increasing along the years \cite{}. Two comprenhensive reviews on \textit{context-based} approaches in \cite{Knees2013, Schedl2014}.
%Another interesting application of NLP is the analysis of music artist-related microblogging posts for artist similarity estimation and artist labeling \cite{Schedl2013a}. More de

There have been also some initial atempts to work with mid and high-level text representations in the context of MIR. In \cite{Sordo2012} a methodology for extracting semantic information from music-related forums is proposed, inferring semantic relations from the co-occurrence of musical concepts in forum posts. In \cite{Sordo2013} a set of semantic facets is automatically obtained and anchored upon the structure of Wikipedia, and tags from the folkosonomy of Last.fm are then categorized with respect to the obtained facets. In \cite{Knees2011} a methodology to automatically extract semantic information and relations about musical entities from arbitrary textual sources is proposed. In \cite{TataandDiEugenio2010} a method to extract information about indidual songs from album reviews is proposed, combining syntactic, semantic and sentiment analysis. Finally, the C@amerata task \cite{sutcliffe2016c, sutcliffe2015}, part of the MeidaEval evaluation campaigns from 2013 to 2017, is focused on music Question & Answering (Q&A) systems. In this task the input is a natural language phrase, together with a music score in MusicXML, and the required output should be one or more matching passages in the score.

In the first and second parts of this thesis, we focus on this latter almost underexplored area. On the one hand, we work on new methodologies for the extraction of high level semantic features from musical texts. On the other hand, we put enphasis on the development of approaches that exploits these high level semantic features in MIR tasks, such as music recommendation, similarity and classification. In addition, we study how semantic information may impact musicological studies. Finally, we investigate how these semantic representations can be combined with other data modalities, such as audio and images.

\section{Representation Learning}

The performance of machine learning methods is heavily dependent on the choice of data representation (features) on which they are applied \cite{bengio2013representation}. Features have been traditionally engineered, with preprocessing piplines and data transformations that result in data representations which are effective for machine learning algorithms. Feature engineering compensate the inhability of traditional machine learning algorithms to extract the discriminative information of the data directly. However, it involves a labor-intensive human effort, and also all the different explanatory factors of variation behind the data are not represented. Therefore, hand-crafted features normally do not generalize well. 

Representation learning (or feature learning) is a technique that allows a learning system to automatically discover the variation behind the data directly from raw data. Representation learning can be divided into supervised and unsupervised. Supervised feature learning typically involves a deep neural network, and features are learned using labeled input data. Unsupervised feature learning learns the data representation with unlabeled input data, using matrix factorization, autoencoders or clustering techniques. 

%This approach has been and still is widely applied in MIR. %According to \cite{Humphry}, MIR approaches are typicaly based on a two-stage architecture of feature extraction and semantic interpretation, e.g., classification, regression, clustering, similarity ranking, etc. 
As stated before, MIR approaches are commonly based on a two-stage architecture of feature extraction and semantic interpretation. In this context, data representations are generally obtainid following a traditional feature extraction process, which involves a combination of music domain-knowledge, psychoacoustics, and audio engineering. Huge efforts have been put in the last two decades in the definition and extraction of audio features, which has given rise to comprenhensive software libraries that assamble many of these feature extraction techniques \cite{Essentia, Librosa}. However, as identified in \cite{humphrey2012}, MIR approaches could benefit from the use of representation learning learning approaches over deep learning architectures. This approach has two main advantages. First, blurring the boundaries between the two-stage architecture, which implies fully-automated optimization of both stages at once. Second, it results in general-purpose architecture that can be applied to different MIR problems and using different data modalities. In the last years, several works have been published where end-to-end learning approaches using deep learning architectures have been applied to MIR tasts such as music recommendation \cite{Oord2013} and music classification \cite{Choi2016}, among others.

%In this thesis, we focus on both types of representation learning methods, and more specifically in supervised deep neural networks and matrix factorization techniques.
In the third part of this disertation, we focus on representation learning approaches based on deep learning architectures that deals with and combine data from different modalities (audio, text and images), which are applied to music classification and music recommendation tasks.

\section{Annotation and Classification of Music Collections}

The advent of large music collections has possed the challenge of how to access the information - in terms of retrieval, browsing, and recommendation -. One way to ease the access of large music collections is to keep annotations of all music resources \cite{sordo2012semantic}. Annotations can be added either manually or automatically. Manual annotation of huge music collections is too costly due to high human effort required. Therefore, the implementation of automatic annotations processes have become mandatory. 
%According to \cite{Pachet2005}, music descriptors can be classified in three groups: editorial metadata, cultural metadata, and acoustic metadata. 

We distinguish two ways to enhance annotations: gathering metadata from external sources or learning metadata from the collection data. The former approach may be addressed by obtaining data from online knowledge repositories or by extracting structured information from collections of unstructured documents. These techniques impose the challenge of properly map collection items with external entities. The latter approach may be addressed by applying machine learning techniques over the data within the collection. When annotations are learned from the audio content this task is called auto-tagging in the MIR literature. However, metadata may be learned from different data modalities, such as album cover artwork, tags, editorial metadata, user generated content, video clips, etc.
Among the different possible annotations of musical items, the most prototypical are music genres, instruments or moods. Among them, we focus in this dissertation in the classification of music genres.
Music genre labels are useful categories to organize and classify songs, albums and artists into broader groups that share similar musical characteristics. They have been widely used for music classification, from physical music stores to streaming services. Automatic music genre classification thus is a widely explored topic \citep{sturm2012survey}.
However, almost all related work is concentrated in multi-class classification of music items into broad genres (e.g. Pop, Rock, Electronic), assigning one unique genre label per item. However, music genres are not mutually exclusive. A song may be Pop, and at the same time have elements from Deep House and a Reggae grove. 
Music classification of few broad genres have been an interesting computational problem for a while, but now it is time to move forward to a more real problem like multi-label genre classification of very specific genres.

In this thesis, we focus in the problem of enriching metadata of music collections from the two above defined standpoints, i.e. gathering and learning. We study how semantic technologies may be useful to improve the annotations of musical items. In addition, we tackle the problem of single-label and multi-label music genre classification from different data modalities (i.e. audio, text and images) and their combination.


\section{Music Recommendation}

Information overload in modern Web applications challenges users in their decision-making tasks. Recommender systems have emerged in the last years as fundamental tools in assisting users to find, in a personalized manner, what is relevant for them in overflowing knowledge spaces. 

Music Recommendation is a relatively young but continuously growing research topic, in both MIR and RecSys communities~\cite{oscarBook}. Several research approaches and commercial systems have been proposed in the last decade. However, most of them are adaptations from other domains \cite{oscarBook}. Music has its own specificities with respect to other domains. For instance, a user may consume an item several times, or very different items according to the user context (e.g. working, dinning, excersising). Therefore, music recommendation is a challenging and still unsolved problem.

Although music online services make available almost all existing music, only a small percentage of these catalogues is actually consumed by the vast majority of users. In fact, most of the existing music is barely consumed. Music consumption follows what is called a long tail distribution \cite{oscarBook} (see Figure~\ref{}). Therefore, one of the main challenges in music recommendation is how to make this long tail of musical items profitable. As music creation is continuously growing, new artists and releases appear every day. Hence, another importat challenge in recommender systems is how to deal with these new items. This scenario is often called the cold-start problem.

Most research in Music Recommendation has been dedicated to developing algorithms that provide \textit{good} and \textit{useful} recommendations~\cite{oscarBook}, neglecting the importance of the diversity or serendipity of recommendations \cite{TODO}. In addition, very few approaches are able to provide explanations of the recommendation to the users~\cite{Passant2008, Passant2010}. According to~\cite{celma2008new}, giving explanations of the recommendations provides transparency to the recommendation process and increases the confidence of the user in the system.

%Within the recommender systems arena, there are two main approaches for computing recommendations: collaborative filtering (CF) and contend-based ones.
%The most popular is collaborative filtering which provides recommendations to a user by considering the preferences of other users with similar tastes. 
%CF methods suffer from the cold-start problem, as new items do not have feedback information \cite{Saveski2014}. Content-based methods \cite{Mooney1999} rely only on item features, and recommendations are based on similarity between such features. Finally, hybrid methods \cite{Burke2002} try to combine both item content and item-user feedback.

%TODO: Background on knowledge-based music recommendation (TODO: cite Netflix, cite w2v).

%TODO: Background on content-based music recommendation (TODO: cite Van den Oord).

%Social tags have been extensively used as a source of artist content features to recommend music \cite{Knees2013}. However, these tags are usually collectively annotated, which often introduce an artist popularity bias \cite{Turnbull2008}.
%Este problema también pasa con las biografías, también son colaborative effort
%Artist biographies and press releases are, on the other hand, a more reliable source of context information. They may be produced by artists themselves, or collectively annotated. 
%Artist biographies and press releases, on the other hand, do not necessarily require a collaborative effort, as they may be produced by artists themselves. 
%However, they have seldom been exploited for music recommendation \cite{Oramas2015}.
%Part of this work focuses on learning artist features from these biographies.

The web is full of documents, knowledge repositories and user generated content with relevant information about music and musicians. This information may have the potential to impact in the performance of music recommender systems. In addition, up to now, audio content has been barely exploited in comercial recommender systems, as its perfomance was not that satisfactory. However, thanks to the advent of novel deep learning approaches \cite{Oord2013}, we believe that audio content may be a key factor to provide more accurate long tail recommendations.
In this thesis we take advantage of this scenario from different perspectives. First, we investigate how information extracted from large datasets of documents \textit{talking} about music may be useful to provide explanations of recommendations to users. Second, we tackle the problem of recommending long tail items by leveraging semantic information from knowledge repositories and combining them with users feedback data. Finally, we address the problem of cold-start music recommendations by combining different data modalities in a multimodal deep learning approach.%, where data representations are directly learned from the data. 


\section{Objectives and outline of the thesis}

In the previous sections we have explained the motivations, described the context and introduced the focus of our thesis. In accordance with all that has been said, the main goal of this thesis is to contribute to advancing the state of the art in music recommendation and music classification by leveraging knowledge repositories and user generated content and by learning data representations from multimodal data. Even though this thesis is focused on the music domain, the work we present strives for generalization to other multimedia domains, and thus can be of interest to researchers working in other fields. Figure\ref{} shows a conceptual organisation of some of the chapters of this thesis according to the different approaches, tasks, and the involved level of learnig. What follows is a brief description of the structure of the thesis along with the specific goals and achievements that are reported in each chapter.
