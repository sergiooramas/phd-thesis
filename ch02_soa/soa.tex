%!TEX root = ../thesis_a4.tex

\chapter{Background}
\label{sec:SOA}

\section{Introduction}
\label{sec:SOA:Introduction}

The literature review presented in this chapter is divided into two main parts.
Firstly, we summarise existing work on information extraction, with special focus on its application to the music domain.
We describe what Entity Linking is, the different state-of-the-art systems. Additionally, we breafly descrive the different approaches for relation extraction. discuss problems that are typically found in tagging systems and highlight some of the solutions that are commonly proposed. 
Secondly, we define what a Knowledge Base (KB) is and the different types of KBs. Moreover, we deepen into the available KBs with music related information.
Thirdly, we focus on existing literature about music genre classification, with a special focus in text-based approaches.
Finally, we outline the different apporaches that have been proposed about semantic-based recommender systems and cold-start music recommendation.


\section{Natural Language Understanding}
\label{sec:SOA:nlu}

Natural language understanding (NLU) is a subtopic of Natural Language Processing (NLP) that deals with machine reading comprehension.
Knowledge Representation and Reasoning is a key enabler of Intelligent Systems \cite{Suchaneketal2007}, and plays an important role in Natural language Understanding (NLU) \cite{BaralandDeGiacomo2015}.
In this disertation, we focus on an important aspect of NLU, which is \textit{how to make sense} of the data that is generated and published online on a daily basis. This data is mostly produced in human-readable format, which makes it unsuitable for automatic processing. Considering that deep understanding of natural language by machines seems to be very far off \cite{CambriaandWhite2014}, there is great interest in formalizing unstructured data, and Knowledge Bases (\textsc{KBs}) are a paradigmatic example of large-scale content processed to make it machine readable.

We may define a \textsc{KB} as a repository of knowledge organized in a predefined taxonomic or ontologic structure, potentially compatible with other \textsc{KBs}, thus contributing to the Linked Open Data initiative\footnote{\url{http://linkeddata.org/}}. These \textsc{KBs} may be designed to represent unconstrained knowledge, or a single domain of interest. This representation is formalized either manually, automatically, or with a combination of both.

Information Extraction (IE) is the task of automatically extracting structured information from unstructured or semi-structured text sources. It is a widely studied technique within the Natural Language Processing (NLP) research community \cite{cowie1996information}.
A major step towards understanding language is the extraction of meaningful terms (entities) from text as well as relationships between those entities. This statement involves two different tasks. The former is to determine the identity and category of entity mentions present in text. This task is called Named Entity Recognition (NER). However, when this task involves a latter step of disambiguation of entities against a KB it is often called Named Entity Disambiguation (NED) or Entity Linking (EL). The second task is to identify and annotate relevant semantic relations between entities in text. This task is called Rellation Extraction.

The work described in this thesis strongly focuses on the exploitation of linguistic and semantic properties of text collections for the automatic learning of Music Knowledge Bases \textsc{MKBs}. For this reason, we deem relevant to cover related work in the following areas: (1) \textsc{KB} learning and curation, with special focus on \textsc{RE} methods; and (2) \textsc{KB} learning and its applications in the music domain as well as Music Information Retrieval (MIR).


\subsection{Knowledge Base Construction}
\label{sec:SOA:nlu:kbs}

We understand language by making sense of the connections between words, concepts, phrases and thoughts \citep{Havasietal2007}. \textsc{KBs} constitute a resource for encapsulating this knowledge. Previous efforts on \textsc{KB} construction may be characterized as: (1) Hand-crafted \textsc{KBs}; (2) Integrative projects (automatic in design, but reliant on manually validated data); and (3) Fully automatic, also in the \textsc{RE} process.

Among the first group, the best known is probably \textsc{WordNet} \citep{Miller1995}, a lexical database which groups concepts in ``synonym sets'', and encodes predefined relations among them such as \textit{hyponymy/hypernymy}, \textit{meronymy}, \textit{holonymy}, or \textit{instantiation}. Manually constructed \textsc{KBs}, however, are mostly developed in specific domains, where the degree of ambiguity is lower and there is more availability of trained knowledge engineers.

Next, integrative projects are probably the most productive, as they are the most ambitious attempts in terms of content coverage and community involvement, not only users, but also contributors. Examples of these include \textsc{Yago} \citep{Suchaneketal2007}, an automatically created \textsc{KB} derived from integrating \textsc{Wikipedia} and \textsc{WordNet}; \textsc{DBpedia} \citep{Lehmanetal2014}, a collaboratively maintained project aimed at exploiting information present in \textsc{Wikipedia}, both structured and in free text; \textsc{Freebase} \citep{Bollacketal2008}, also a collaborative effort mainly based on extracting structured knowledge from \textsc{Wikipedia}; or \textsc{BabelNet} \citep{NavigliPonzetto2012}, a semantic network which started as a seamless integration of \textsc{Wikipedia} and WordNet, and today constitutes the largest multilingual repository of words and senses.

With regard to the third group we refer to approaches where knowledge is obtained automatically. Usually, these are framed within the \textit{Open Information Extraction} (OIE) paradigm \citep{Bankoetal2007}, which can be (roughly) summarized as (1) reading the web, (2) learning facts, (3) scoring them; and (4) structuring them according to predefined semantic criteria. Endeavours in this area include \textsc{TextRunner} \citep{Bankoetal2007}, widely regarded as the first OIE system; \textsc{ReVerb} \citep{Fader2011}, particularly designed to reduce noise while keeping a wide coverage, thanks in part to a set of syntactic and lexical constraints; \textsc{NELL} \citep{Carlson2010}, which incorporates semantic knowledge in the form of a hand-crafted taxonomy of entities and relations; \textsc{PATTY} \citep{Nakasholeetal2012} and \textsc{WiseNet} \citep{MoroandNavigli2012,MoroandNavigli2013}, in which a shared vision to integrate semantics is applied both at the entity and relation level; \textsc{DefIE} \citep{DelliBovietal2015b}, a recent development in OIE tested on the whole set of \textsc{BabelNet} glosses; and \textsc{KB-Unify} \citep{DelliBovietal2015}, not an actual OIE implementation, but rather a unification framework for OIE systems.

%Another key aspect of automatic \textsc{KB} generation, in addition to semantics, is \textit{how relations between entities are captured}. From rule-based linguistically motivated approaches to machine learning methods, the general trend seems to extract as many facts as possible, with as much accuracy as possible, and keeping the degree of supervision low.

%Previous work exploited combinations of surface and part-of-speech patterns \citep{Bankoetal2007} or regular expressions \citep{Fader2011}, as well as rules based on shallow parsing \citep{MoroandNavigli2012}. Furthermore, there are a number of contributions exploiting syntactic information in the form of syntactic dependencies, a linguistic formalism \citep{Tesniere1959} that represents sentences as trees where each relation is bi-lexical and non phrasal, and where in general, syntactically important words (subject, verb, or direct object) appear higher in the tree. Syntactic dependencies have been extensively used in \textsc{RE}, e.g. in supervised machine learning settings for computational lexicography \citep{EspinosaandSaggion2014}. They have played a role also in Entity Linking (EL), e.g. enabling syntactic tree traversal between entities \citep{BunescuandMooney2005}, providing smallest spanning subtrees subsuming two entities \citep{CulottaSorensen2004}, or as part of a rule-based OIE system \citep{Gamallo2012}.

\subsection{Music Knowledge Bases}\label{sec:SOA:nlu:mkbs}

\textsc{MusicBrainz} and \textsc{Discogs} are two paramount examples of manually curated \textsc{MKBs}. They are open music encyclopedias of music metadata built collaboratively and openly available. \textsc{MusicBrainz}, in addition, is regularly published as Linked Data by the \textsc{LinkedBrainz} project\footnote{\url{http://linkedbrainz.org/}}.

As for generic \textsc{KBs} based on \textsc{Wikipedia}, such as the ones described earlier, these include a remarkable amount of music data, such as artist, album and song biographies, definitions of musical concepts and genres, or articles about music institutions and venues. However, their coverage is biased towards the best known artists, and towards products from Western culture. Finally, let us refer to the notable case of \textsc{Grove Music Online}\footnote{\url{http://www.oxfordmusiconline.com}}, a music encyclopedia containing over 60k articles written by music scholars. However, it has the drawback of not being freely open, as it runs by subscription.

Other than the aforementioned curated repositories, to the best of our knowledge, there is not a single automatically learned open \textsc{MKB}. A first step in this direction was taken in \citep{Sordo2015,Oramas2014}, applying \textsc{RE} techniques to big datasets of music related texts extracted from the web. Moreover, in \citep{Oramas2015b}, a Flamenco \textsc{MKB} is created by combining data from curated \textsc{KBs} and information extracted from blogs and websites.

Despite their scarcity, \textsc{MKBs} are becoming increasingly popular in MIR applications, such as artist similarity and music recommendation \citep{Celma2008,Oramas2015a,Leal2012,Ostuni2015}. 
MKBs have also been exploited as sources of explanations in music recommender systems. According to~\citep{CelmaandHerrera2008}, giving explanations of the recommendations provides transparency to the recommendation process and increases the confidence of the user in the system. In \citep{Passant2010}, explanations of recommendations are created by exploiting \textsc{DBpedia}'s structured information, whilst in \citep{Sordo2015}, explanations are based on an automatically learned \textsc{MKB}. 


\subsection{Entity Linking}
\label{sec:SOA:nlu:entity_linking}


The advent of large knowledge repositories and collaborative resources has contributed to the emergence of Entity Linking (EL), i.e. the task of discoveing mentions of entities in text and link them to a suitable knowledge repository \citep{Moroetal2014}. 
It encompasses similar subtasks such as Named Entity Disambiguation \citep{BunescuandPasca2006}, which is precisely linking mentions to entities to a KB, or Wikification \citep{MihalceaandCsomai2007}, specifically using Wikipedia as KB.
There have been a great development of EL systems that perform well in general purpose domains. Among these systems we focus on three of them in this thesis:

\noindent \textbf{DBpedia Spotlight} \citep{Mendes2011} is a system for automatically annotating text documents with DBpedia URIs, finding and disambiguating natural language mentions of DBpedia resources. DBpedia Spotlight is shared as open source and deployed as a Web service freely available for public use\footnote{lhttps://github.com/dbpedia-spotlight/dbpedia-spotlight/wiki/Web-service}.
DBpedia Spotlight gives as a result the DBpedia uri, start and end char positions, the value of the rdf:type property, and a confidence score.\\

\vspace{-0.2cm}
\noindent \textbf{TagMe} \citep{Ferragina2012} is an EL system that matches terms with Wikipedia link texts and disambiguates them using the in-link graph and the page dataset. Then, it performs a pruning process by looking at the entity context. TagMe is available as a web service~\footnote{lhttp://tagme.di.unipi.it/}.
Tagme output provides the start and end char position, the Wikipedia page id, the Wikipedia categories and a confidence score. \\

\vspace{-0.2cm}
\noindent \textbf{Babelfy} \citep{Moroetal2014b} is an EL and WSD based on non-strict identification of candidate meanings (i.e. not necessarily exact string matching), together with a graph based algorithm that traverses the BabelNet graph and selects the most appropriate semantic interpretation for each candidate.
Babelfy output provides the BabelNet synset and the word index. If the synset references to a Wikipedia page, it returns the Wikipedia url, the DBpedia uri and the Wikipedia categories. If it points to WordNet, it yields the name of the equivalent WordNet synset.

In the context of Open Data, the need for benchmarking datasets and evaluation frameworks for EL is clear. However, while general purpose datasets exist \citep{Usbeck2015}, dealing with highly specific domains (e.g. chemistry) or ever-evolving areas (e.g. videogames or music) poses a greater challenge due to linguistic idiosincrasies or under-representation in general purpose knowledge-bases. This is true in the music domain as well, where available data is scarce \citep{Gruhl2009}.

Among the few works on EL for the music domain, let us refer to \citep{Gruhl2009}, who describe an approach for detecting musical entities in informal text. In addition, \citep{Zhang2009} describe a system for musical EL in the Chinese language based on Hidden Markov Models. Finally, \citep{Oramas2015} describe an EL system for recognizing musical entities in a relation extraction pipeline.

Regarding unification frameworks for EL, \citep{Cornolti2013} put forward a benchmarking framework for comparing EL systems, leveraging Wikipedia, and a hierarchy of EL problems together with a set of novel measures. \citep{Rizzo2014} divide the process into NER and EL, evaluating each stage individually. They subsequently describe a system aimed at combining the output of the different NER systems. Finally \citep{Usbeck2015} present GERBIL, an evaluation framework for semantic EL based on \citep{Cornolti2013}. It is an open-source and extensible framework that allows evaluating tools against different datasets.


\subsection{Relation Extraction}
\label{sec:SOA:nlu:relation_extraction}

%TODO-LUIS: go through this section one more time [a little bit about relation extraction (plus maybe NER, dependency parsing) (check the review paper by Nguyen Bach and Sameer Badaskar, ``A Review of Relation Extraction'' \citep{Bach2007})]

%Vast amount of unstructured text on the web. One way to ... is to structure the text by annotating in such a way that it can be use/manipulated by algorithms/machines?

A large portion of the knowledge contained in the web is stored in unstructured natural language text. In order to acquire and formalize this heterogeneous knowledge, methods that automatically process this information are in demand. Extracting semantic relations between entities is an important step towards this formalization \citep{Wang2008}. Relation Extraction is an established task in Natural Language Processing~\citep{Bach2007}. It has been defined as the process of identifying and annotating relevant semantic relations between entities in text \citep{JiangZhai2007}. 

Relation Extraction (RE) approaches are often classified according to the level of supervision involved.
%Relation Extraction systems are often classified into Traditional and Open Relation Extraction.
%In traditional Relation Extraction (RE), the vocabulary of extracted relations is defined a priori, i.e. in a domain ontology or an extraction template.
Supervised learning is a core-component of a vast number of RE systems, as they offer high precision and recall. However, the need of hand labeled training sets makes these methods not scalable to the thousands of relations found on the Web \citep{Hoffmann2011}.
%semi-supervised and bootstrapping approaches have gained popularity \citep{Bach2007}.
More promising approaches, called semi-supervised approaches, bootstrapping approaches, or distant supervision approaches do not need big hand labeled corpus, and
%do not need a complete hand labeled training corpus. These approaches
often rely on existent knowledge base to heuristically label a text corpus (e.g.,~\citep{Carlson2010,Hoffmann2011})
%, and use it to learn a probabilistic model ( (such as~\citep{Hoffmann2011}) ) . A similar approach is used by the NELL system \citep{Carlson2010}, but instead of learning a probabilistic model, it is based on a bootstrapping approach.
%Multi-instance learning approaches are combined with distant supervision to combat the problem of ambiguously-labeled training data for the identification of overlapping relations \citep{Hoffmann2011, Xu2013}.
%This is because supervised training of accurate relation extractors is costly, requiring a high number of annotated examples.
%, or it is learned from hand tagged training examples.
%In contrast with traditional Relation Extraction,
Open Information Extraction methods do not require a pre-specified vocabulary, as they aim to discover all possible relations in the text \citep{Banko2007}.
%A small set of labeled data is necessary to train Open IE systems, as they rely on self-supervised learning approaches \citep{Mausam2012}.
However, these methods have to deal with uninformative and incoherent extractions. In ReVerb \citep{Fader2011} part-of-speech based regular expressions are introduced to reduce the number of these incoherent extractions. Less restrictive pattern templates based on dependency paths are learned in OLLIE \citep{Mausam2012} to increase the number of possible extracted relations.
%Extracting semantic relations between entities is an important step towards this formalization \citep{Wang2008}. Relation Extraction is an established task in Natural Language Processing~\citep{Bach2007}. It has been defined as the process of identifying and annotating relevant semantic relations between entities in text \citep{JiangZhai2007}. It typically consists of two steps. First, entities are identified in the text. The identification can be achieved by using a well known NLP technique called Named Entity Recognition. Named Entity Recognition (NER) is the process of identifying proper nouns in running text, such as persons, locations or organizations \citep{Bach2007}. This problem has been studied extensively and the most commonly proposed solutions use linguistic grammar based techniques or machine learning approaches~\citep{Nadeau2007}. The latter step in Relation Extraction involves the indentification of a semantic relation between two named entities. Supervised approaches either rely heavily on features obtained from careful linguistic analysis, like Part-of-Speech tagging or syntactic parsing, or exploit a structured representation of natural language (e.g. sequences or trees) \citep{JiangZhai2007}. Although supervised learning is a core-component of a vast number of RE systems, semi-supervised and bootstrapping approaches have gained popularity \citep{Bach2007}. This is because supervised training of accurate relation extractors is costly, requiring a high number of annotated examples.
%Semi-supervised methods, on the other hand, only need a small number of labeled examples, which in some cases can cause a drop in accuracy \citep{Carlson2010}.
%In Traditional Relation Extraction, the vocabulary of extracted relations is defined a priori, i.e. in a domain ontology or an extraction template, or it is learned from hand tagged training examples. On the other hand, Open Relation Extraction methods do not require a pre-specified vocabulary, as they aim to discover all possible relations in the text \citep{Banko2007}. However, these methods have to deal with uninformative and incoherent extractions. In ReVerb \citep{Fader2011} part-of-speech based regular expressions are introduced to reduce the number of these incoherent extractions. Less restrictive pattern templates based on dependency paths are learned in OLLIE \citep{Mausam2012} to increase the number of possible extracted relations.
Unsupervised approaches do not need any annotated corpus. In \citep{Eichler2008} verb relations involving a subject and an object are extracted, using simplified dependency trees in sentences with at least two named entities. These approaches can process very large amounts of data, however, the resulting relations are hard to map to ontologies \citep{Augenstein2014}.
%The aim of this paper is to show that when the analyzed unstructured text sources are domain specific (in this case music) and reviewed by a group of domain experts, unsupervised approaches using simple rules can map extracted relations with an existing knowledge base with a high precision.

% There are some approaches about how to extract those relations, but generally they formulate the task as a binary classification problem.

%The above examples refer to Traditional Relation Extraction, where relations to be extracted should be manually defined a priori from text. On the other hand, Open Relation Extraction methods do not require to manually define the relations to be extracted, however, they use to have lower recall. Both methods can be also combined, extracting relations patterns automatically and using those extracted pattern as input of Traditional Relation Extraction systems \citep{Mohamed2011}.

%One of these structured representations can be obtained from Dependency Parsing, which provides a tree-like syntactic structure of a sentence based on the linguistic theory of
%Another way of obtaining relations between entities in the text is by using a technique called Dependency Parsing.
In our work we use an NLP task called Dependency Parsing in the relation extraction process. Dependency Parsing provides a tree-like syntactic structure of a sentence based on the linguistic theory of Dependency Grammar \citep{Tesniere1959}. One of the outstanding features of Dependency Grammar is that it represents binary relations between words \citep{BallesterosNivre2013}, where there is a unique edge joining a node and its parent node (see Fig. \ref{fig:sampletree} for the full parsing of an example sentence).
Dependency relations have been successfully incorporated to RE systems. For example, \citep{BunescuandMooney2005} describe and evaluate a RE system based on shortest paths among named entities. \citep{CulottaSorensen2004} focus on the smallest dependency subtree in the sentence that captures the entities involved in a relation, and \citep{Gamallo2012} propose a rule-based dependency-parsing Open IE system. Moreover, in \citep{Nakasholeetal2012,MoroandNavigli2012,DelliBovietal2015b} syntactyc and semantic information is combined by means of the combination of Dependency Parsing and Entity Linking techniques.


\section{Music Information Retrieval}
\label{sec:SOA:mir}

Music Information Retrieval (MIR) is a multidisciplinary field of research that is concerned with the extraction, analysis, and usage of information about music. Traditionally, MIR has been more focused on the use of audio content, underestimating other sources of information. However, in recent years several studies have showed the benefits of using other modalities, as well as their combination in multimodal approaches \cite{Schedl2014}. 


\subsection{Music Genre Classification}
\label{sec:SOA:mir:classfication}

Music genre labels are useful categories to organize and classify songs, albums and artists into broader groups that share similar musical characteristics. They have been widely used for music classification, from physical music stores to streaming services. Music genre classification thus is a widely explored topic within the MIR community \citep{sturm2012survey}.

Most published music genre classification approaches rely on audio sources \citep{sturm2012survey,bogdanov2016cross}. 
Traditional techniques typically use handcrafted audio features, such as Mel Frequency Cepstral Coefficients (MFCCs) \citep{logan2000mel}, as input of a machine learning classifier (e.g., SVM, k-NN) \citep{Tzanetakis2002,seyerlehner2010using}.
More recent deep learning approaches take advantage of visual representations of the audio signal in form of spectrograms.
These visual representations of audio are used as input to Convolutional Neural Networks (CNNs) \citep{dieleman2011audio,dieleman2014end,pons2016experimenting,Choi2016,choi2016convolutional}, following approaches similar to those used for image classification.

Text-based approaches have also been explored for this task. For instance, one of the earliest attempts on genre classification of music reviews is described in \citep{Hu2005}, where experiments on multiclass genre classification and star rating prediction are described. Similarly, \citep{Hu2006} extend these experiments with a novel approach for predicting usages of music via agglomerative clustering, and conclude that bigram features are more informative than unigram features. Moreover, part-of-speech (POS) tags along pattern mining techniques are applied in \citep{Downie2006} to extract descriptive patterns for distinguishing negative from positive reviews. Additional textual evidence is leveraged in \citep{Choi2014}, who consider lyrics as well as texts referring to the meaning of the song, and used for training a kNN classifier for predicting song subjects (e.g. war, sex or drugs).

%In \citep{Mullen2004}, a dataset of music reviews is used for album rating prediction by exploiting features derived from sentiment analysis. First, music-related topics are extracted (e.g. artist or music work), and this topic information is further used as features for classification. One of the most thorough works on music reviews is described in \citep{Tata2010}. It applies Natural Language Processing (NLP) techniques such as named entity recognition, text segmentation and sentiment analysis to music reviews for generating texts explaining good aspects of songs in recommender systems. In the line of review generation, \citep{Ellis2004} combine text analysis with acoustic descriptors in order to generate new reviews from the audio signal. Finally, semantic music information is used in \citep{Zheng2011} to improve topic-wise classification (album, artist, melody, lyrics, etc.) of music reviews using Support Vector Machines. This last approach differs from ours in that it enriches feature vectors by taking advantage of \textit{ad-hoc} music dictionaries, while in our case we take advantage of Semantic Web resources.

By contrast, there are a limited number of papers dealing with image-based genre classification \citep{libeks2011you}.
Regariding multimodal approaches found in the literature, most of them combine audio and song lyrics as text \citep{laurier2008multimodal,neumayer2007integration}. 
Moreover, other modalities such as audio and video have been explored \citep{schindler2015audio}. 

Almost all related work about Music Genre Classification is concentrated in multi-class classification of music items into broad genres (e.g., Pop, Rock), assigning a single label per item. This is problematic since there may be hundreds of more specific music genres \citep{pachet2000taxonomy}, and these may not be necessarily mutually exclusive (i.e., a song could be Pop, and at the same time have elements from Deep House and a Reggae grove). 
Multi-label classification is a widely studied problem \citep{tsoumakas2006multi,jain2016extreme}. 
Although there are not many approaches for multi-label classification of music genres \citep{Sanden2011,wang2009tag}, there is a long tradition in MIR for tag classification, which is a highly related multi-label problem \citep{Choi2016,wang2009tag}.


\subsection{Artist Similarity}
\label{sec:SOA:mir:similarity}

Music artist similarity has been studied from the score level, the acoustic level, and the cultural level \citep{Ellis2002}. In this disertation, we focus on the latter approach, and more specifically in text-based approaches. Literature on document similarity, and more specifically on the application of text-based approaches for artist similarity is discussed next.

The task of identifying similar text instances, either at sentence or document level, has applications in many areas of Artificial Intelligence and Natural Language Processing \citep{LiuandWang2014}. In general, document similarity can be computed according to the following approaches: surface-level representation like keywords or n-grams \citep{ChimandDeng2008}; corpus representation using counts \citep{Rorvig1999}, e.g. word-level correlation, jaccard or cosine models; Latent factor models, such as Latent Semantic Analysis \citep{Deerwesteretal1990}; or methods exploiting external knowledge bases like ontologies or encyclopedias \citep{Huetal2009}.

The use of text-based approaches for artist and music similarity was first applied in \citep{Cohen2000}, by computing co-occurrences of artist names in web page texts and building term vector representations. By contrast, in \citep{Schedl2005} term weights are extracted from search engine's result counts. In \citep{Whitman2002} n-grams, part-of-speech tagging and noun phrases are used to build a term profile for artists, weighted by employing tf-idf. Term profiles are then compared and the sum of common terms weights gives the similarity measure. %In a similar approach \citep{Knees2004}, only unigrams are used and cosine similarity is applied to compute the resemblance between term profiles.
More approaches using term weight vectors have been developed over different text sources, such as music reviews \citep{Hu2005}, blog posts \citep{Celma2006}, or microblogs \citep{Schedl2013}.
In \citep{Logan2003} Latent Semantic Analysis is used to measure artist similarity from song lyrics. Domain specific ontologies have also been applied to the problem of music recommendation and similarity, such as in \citep{Celma2008}. In \citep{Leal2012}, paths on an ontological graph extracted from DBpedia are exploited for recommending music web pages. However, to the best of our knowledge, there are scant approaches in the music domain that exploit implicit semantics and enhance term profiles with external knowledge bases.


\subsection{Recommender Systems}
\label{sec:SOA:mir:recommendation}

Recommender systems can be broadly classified into collaborative filtering (CF), content-based, and hybrid methods. Collaborative filtering methods \citep{Koren2009} use the item-user feedback matrix and predictions are based on the similarity of user or items profiles. Matrix factorization techniques are currently CF state-of-the-art \citep{Koren2009}. 
CF methods suffer from the cold-start problem, as new items do not have feedback information \citep{Saveski2014}. Content-based methods \citep{Mooney1999} rely only on item features, and recommendations are based on similarity between such features. Finally, hybrid methods \citep{Burke2002} try to combine both item content and item-user feedback.


\subsubsection{Semantic-based Approaches}
\label{sec:SOA:mir:recommendation:semantic}

Ontology-based and semantics-aware recommendation systems have been proposed in many works in the past. 
%The usage of ontologies to improve recommendation systems has been proposed in many works in the past. 
In \citep{Middleton_2009} an ontological recommender system is presented that makes use of semantic user profiles to compute collaborative recommendations with the effect of mitigating cold-start and improving overall recommendation accuracy. 
In \citep{mobasher2004} the authors present a \textit{semantically enhanced collaborative filtering} approach, where structured semantic knowledge about items is used in conjunction with user-item ratings to create a combined similarity measure for item comparisons. 
In \citep{Ziegler2004} taxonomic information is used to represents the user's interest in categories of products. Consequently, user similarity is determined by common interests in categories and not by common interests in items. 
In \citep{Anand2007} the authors present an approach that infers user preferences from rating data using an item ontology. The system collaboratively generates recommendations using the ontology and infers preferences during similarity computation. 
Another hybrid ontological recommendation system is proposed in \citep{Cantador08amultilayer} where user preferences and item features are described by semantic concepts to obtain users' clusters corresponding to implicit \textit{Communities of Interest}.
In all of these works, the experiments prove an accuracy improvement over traditional memory-based collaborative approaches especially in presence of sparse datasets. 
In the last few years with the availability of Linked Open Data (LOD) datasets, a new class of recommender systems has emerged which can be named as LOD-based recommender systems. 
One of the first approaches that exploits Linked Open Data for building recommender systems is \citep{HeitmannH10}. 
In \citep{Fernandez-Tobias2011} the authors present a knowledge-based framework leveraging DBpedia for computing cross-domain recommendations. 
In \citep{DMOR12,DMORZ12} a model-based approach and a memory-based one to compute content-based recommendations are presented leveraging LOD datasets. Another LOD content-based method is presented in \citep{ODMD14a} which defines a neighborhood-based graph kernel for matching graph-based item representations. 
Two hybrid approaches have been presented lately. In \citep{Ostuni2013} the authors show how to compute top-N recommendations from implicit feedback using linked data sources and in \citep{Khrouf2013} the authors propose an event recommendation system based on linked data and user diversity. 
In \citep{Rowe1} the authors propose a semantic-aware extension of the SVD++ model, named SemanticSVD++, which incorporates semantic categories of items into the model. The model is able also to consider the evolution over time of user's preferences. 
Finally, another interesting direction about the usage of LOD for content-based RSs is explored in \citep{MustoSLG14} where the authors present Contextual eVSM, a content-based context-aware recommendation framework that adopts a semantic representation based on distributional models and entity linking techniques. In particular entity linking is used to detect entities in free text and map them to LOD.


\subsubsection{Music Recommendation}
\label{sec:SOA:mir:recommendation:music}

An overview about techiques for music recommendation and similarity based on music contextual data is given in \citep{Knees2013}. 
In \citep{KaminskasR12} the authors provide a description of various tools and techniques that can be used for addressing the research
challenges posed by context-aware music retrieval and recommendation. 
A survey about techniques for the generation of music playlists is given in \citep{Bonnin2014}. In particular, the authors provide a review of the literature on automated playlist generation and a categorization of the existing approaches. 
A context-aware music recommender system which infers contextual information based on the most recent sequence of songs liked by the user is presented in \citep{HaririMB12}.
More recently, a playlist generation algorithm with the goal of maximizing coherence and personalization of the playlist has been presented in \citep{Jannach2015}. Finally, in \citep{AghdamHMB15} a technique for adapting recommendations to contextual changes based on hierarchical hidden Markov models is presented.

Social tags have been extensively used as a source of artist content features to recommend music \citep{Knees2013}. However, these tags are usually collectively annotated, which often introduce an artist popularity bias \citep{Turnbull2008}.
%Este problema también pasa con las biografías, también son colaborative effort
%Artist biographies and press releases are, on the other hand, a more reliable source of context information. They may be produced by artists themselves, or collectively annotated. 
Artist biographies and press releases, on the other hand, do not necessarily require a collaborative effort, as they may be produced by artists themselves. 
However, they have seldom been exploited for music recommendation \citep{Oramas2015}.
Part of the work on this disertation focuses on the exploitation of these biographies.


Furthermore, we also make use of audio signals, since these are generally always available and have shown to be helpful when recommending music in the long tail \citep{Oord2013}.
% relevant music information can also be extracted directly from the audio signal, which can help to recommend music (TODO: cite van den oord).

In both cases, a hybrid recommendation approach is used based on learning attribute-to-feature mappings \citep{GantnerDFRS10}.
This method addresses the lack of feedback for uncommon items in two steps: (1) factorizing the collaborative matrix, and (2) learning a mapping between item content features and item latent factors \citep{Oord2013,Bansal2016}.
